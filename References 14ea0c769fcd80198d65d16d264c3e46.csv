Name,Title,Authors,Year,Extra,In-Text Citation,Citation Key,URL,Publication,Short Title,Series Title,Cover,Date,Full Citation,DOI,Zotero URI,Editors,File Path,Tags,Relevance,Problem or Purpose,Theoretical/Conceptual Framework,Sample,Methods,Key Findings,Recommendations,My Comments,Related References,Due Date,Own?,Abstract,Collections,Date Added,Date Modified,Place,Proceedings Title,Item Type,Reading Status,Journal,Volume
"Grattafiori et al., 2024",The Llama 3 Herd of Models,"Grattafiori, Aaron
Dubey, Abhimanyu
Jauhri, Abhinav
Pandey, Abhinav
Kadian, Abhishek et al.",2024,arXiv:2407.21783 [cs],"Grattafiori et al., “The Llama 3 Herd of Models.”",,http://arxiv.org/abs/2407.21783,,,,,2024-11-23,"Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. “The Llama 3 Herd of Models.” arXiv, November 23, 2024. https://doi.org/10.48550/arXiv.2407.21783.",https://doi.org/10.48550/arXiv.2407.21783,https://zotero.org/users/local/bHD6JzYJ/items/6JZ4AZIY,,/Users/slseanwu/Zotero/storage/DQRTYI4S/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition",,,,,,,,,,,,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",67960-DL-project,"December 3, 2024 12:36 AM (UTC)","December 3, 2024 12:37 AM (UTC)",,,Preprint,To Find,,
"Melechovsky et al., 2024",MidiCaps: A large-scale MIDI dataset with text captions,"Melechovsky, Jan
Roy, Abhinaba
Herremans, Dorien",2024,arXiv:2406.02255 [eess],"Melechovsky, Roy, and Herremans, “MidiCaps.”",,http://arxiv.org/abs/2406.02255,,MidiCaps,,,2024-07-22,"Melechovsky, Jan, Abhinaba Roy, and Dorien Herremans. “MidiCaps: A Large-Scale MIDI Dataset with Text Captions.” arXiv, July 22, 2024. https://doi.org/10.48550/arXiv.2406.02255.",https://doi.org/10.48550/arXiv.2406.02255,https://zotero.org/users/local/bHD6JzYJ/items/LH8ER3T9,,/Users/slseanwu/Zotero/storage/598QAYED/Melechovsky et al. - 2024 - MidiCaps A large-scale MIDI dataset with text captions.pdf,"Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",67960-DL-project,"December 3, 2024 6:49 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Thickstun et al., 2024",Anticipatory Music Transformer,"Thickstun, John
Hall, David
Donahue, Chris
Liang, Percy",2024,arXiv:2306.08620 [cs],"Thickstun et al., “Anticipatory Music Transformer.”",,http://arxiv.org/abs/2306.08620,,,,,2024-07-25,"Thickstun, John, David Hall, Chris Donahue, and Percy Liang. “Anticipatory Music Transformer.” arXiv, July 25, 2024. https://doi.org/10.48550/arXiv.2306.08620.",https://doi.org/10.48550/arXiv.2306.08620,https://zotero.org/users/local/bHD6JzYJ/items/4KE8HSED,,/Users/slseanwu/Zotero/storage/FP89YXRC/Thickstun et al. - 2024 - Anticipatory Music Transformer.pdf,"Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning",,,,,,,,,,,,"We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with similar musicality to even music composed by humans over a 20-second clip.",67960-DL-project,"December 3, 2024 6:49 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Agostinelli et al., 2023",MusicLM: Generating Music From Text,"Agostinelli, Andrea
Denk, Timo I.
Borsos, Zalán
Engel, Jesse
Verzetti, Mauro
Caillon, Antoine
Huang, Qingqing
Jansen, Aren
Roberts, Adam
Tagliasacchi, Marco
Sharifi, Matt
Zeghidour, Neil
Frank, Christian",2023,arXiv:2301.11325 [cs],"Agostinelli et al., “MusicLM.”",,http://arxiv.org/abs/2301.11325,,MusicLM,,,2023-01-26,"Agostinelli, Andrea, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, et al. “MusicLM: Generating Music From Text.” arXiv, January 26, 2023. https://doi.org/10.48550/arXiv.2301.11325.",https://doi.org/10.48550/arXiv.2301.11325,https://zotero.org/users/local/bHD6JzYJ/items/PP79PLSH,,/Users/slseanwu/Zotero/storage/UPGG2NA9/Agostinelli et al. - 2023 - MusicLM Generating Music From Text.pdf,"Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"We introduce MusicLM, a model generating high-fidelity music from text descriptions such as ""a calming violin melody backed by a distorted guitar riff"". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.",67960-DL-project,"December 3, 2024 6:50 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Copet et al., 2023",Simple and Controllable Music Generation,"Copet, Jade
Kreuk, Felix
Gat, Itai
Remez, Tal
Kant, David
Synnaeve, Gabriel
Adi, Yossi
Défossez, Alexandre",2024,arXiv:2306.05284 [cs],"Copet et al., “Simple and Controllable Music Generation.”",,http://arxiv.org/abs/2306.05284,,,,,2024-01-30,"Copet, Jade, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. “Simple and Controllable Music Generation.” arXiv, January 30, 2024. https://doi.org/10.48550/arXiv.2306.05284.",https://doi.org/10.48550/arXiv.2306.05284,https://zotero.org/users/local/bHD6JzYJ/items/WVZ8MXJD,,/Users/slseanwu/Zotero/storage/KZUKJ6ZV/Copet et al. - 2024 - Simple and Controllable Music Generation.pdf,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft",67960-DL-project,"December 3, 2024 6:50 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Evans et al., 2024",Stable Audio Open,"Evans, Zach
Parker, Julian D.
Carr, C. J.
Zukowski, Zack
Taylor, Josiah
Pons, Jordi",2024,arXiv:2407.14358 [cs],"Evans et al., “Stable Audio Open.”",,http://arxiv.org/abs/2407.14358,,,,,2024-07-31,"Evans, Zach, Julian D. Parker, C. J. Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. “Stable Audio Open.” arXiv, July 31, 2024. https://doi.org/10.48550/arXiv.2407.14358.",https://doi.org/10.48550/arXiv.2407.14358,https://zotero.org/users/local/bHD6JzYJ/items/WNSF4R39,,/Users/slseanwu/Zotero/storage/VQU4JZKA/Evans et al. - 2024 - Stable Audio Open.pdf,"Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.",67960-DL-project,"December 3, 2024 6:50 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Deng et al., 2024",ComposerX: Multi-Agent Symbolic Music Composition with LLMs,"Deng, Qixin
Yang, Qikai
Yuan, Ruibin
Huang, Yipeng
Wang, Yi
Liu, Xubo
Tian, Zeyue
Pan, Jiahao
Zhang, Ge
Lin, Hanfeng
Li, Yizhi
Ma, Yinghao
Fu, Jie
Lin, Chenghua
Benetos, Emmanouil
Wang, Wenwu
Xia, Guangyu
Xue, Wei
Guo, Yike",2024,arXiv:2404.18081 [cs],"Deng et al., “ComposerX.”",,http://arxiv.org/abs/2404.18081,,ComposerX,,,2024-04-30,"Deng, Qixin, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, et al. “ComposerX: Multi-Agent Symbolic Music Composition with LLMs.” arXiv, April 30, 2024. https://doi.org/10.48550/arXiv.2404.18081.",https://doi.org/10.48550/arXiv.2404.18081,https://zotero.org/users/local/bHD6JzYJ/items/THPXHHN6,,/Users/slseanwu/Zotero/storage/N6NYCAP7/Deng et al. - 2024 - ComposerX Multi-Agent Symbolic Music Composition with LLMs.pdf,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",67960-DL-project,"December 3, 2024 6:53 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Yuan et al., 2024",ChatMusician: Understanding and Generating Music Intrinsically with LLM,"Yuan, Ruibin
Lin, Hanfeng
Wang, Yi
Tian, Zeyue
Wu, Shangda
Shen, Tianhao
Zhang, Ge
Wu, Yuhang
Liu, Cong
Zhou, Ziya
Ma, Ziyang
Xue, Liumeng
Wang, Ziyu
Liu, Qin
Zheng, Tianyu
Li, Yizhi
Ma, Yinghao
Liang, Yiming
Chi, Xiaowei
Liu, Ruibo
Wang, Zili
Li, Pengfei
Wu, Jingcheng
Lin, Chenghua
Liu, Qifeng
Jiang, Tao
Huang, Wenhao
Chen, Wenhu
Benetos, Emmanouil
Fu, Jie
Xia, Gus
Dannenberg, Roger
Xue, Wei
Kang, Shiyin
Guo, Yike",2024,arXiv:2402.16153 [cs],"Yuan et al., “ChatMusician.”",,http://arxiv.org/abs/2402.16153,,ChatMusician,,,2024-02-25,"Yuan, Ruibin, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, et al. “ChatMusician: Understanding and Generating Music Intrinsically with LLM.” arXiv, February 25, 2024. https://doi.org/10.48550/arXiv.2402.16153.",https://doi.org/10.48550/arXiv.2402.16153,https://zotero.org/users/local/bHD6JzYJ/items/RBSIKXPT,,/Users/slseanwu/Zotero/storage/LQVVJYCP/Yuan et al. - 2024 - ChatMusician Understanding and Generating Music Intrinsically with LLM.pdf,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.",67960-DL-project,"December 3, 2024 6:54 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Kilgour et al., 2019",Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms,"Kilgour, Kevin
Zuluaga, Mauricio
Roblek, Dominik
Sharifi, Matthew",2019,arXiv:1812.08466 [eess],"Kilgour et al., “Fréchet Audio Distance.”",,http://arxiv.org/abs/1812.08466,,Fréchet Audio Distance,,,2019-01-17,"Kilgour, Kevin, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. “Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms.” arXiv, January 17, 2019. https://doi.org/10.48550/arXiv.1812.08466.",https://doi.org/10.48550/arXiv.1812.08466,https://zotero.org/users/local/bHD6JzYJ/items/3RGQC88S,,/Users/slseanwu/Zotero/storage/VYGLS9G7/Kilgour et al. - 2019 - Fréchet Audio Distance A Metric for Evaluating Music Enhancement Algorithms.pdf,"Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"We propose the Fr\'echet Audio Distance (FAD), a novel, reference-free evaluation metric for music enhancement algorithms. We demonstrate how typical evaluation metrics for speech enhancement and blind source separation can fail to accurately measure the perceived effect of a wide variety of distortions. As an alternative, we propose adapting the Fr\'echet Inception Distance (FID) metric used to evaluate generative image models to the audio domain. FAD is validated using a wide variety of artificial distortions and is compared to the signal based metrics signal to distortion ratio (SDR), cosine distance and magnitude L2 distance. We show that, with a correlation coefficient of 0.52, FAD correlates more closely with human perception than either SDR, cosine distance or magnitude L2 distance, with correlation coefficients of 0.39, -0.15 and -0.01 respectively.",67960-DL-project,"December 3, 2024 6:54 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"Wu et al., 2023",Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation,"Wu, Yusong
Chen, Ke
Zhang, Tianyu
Hui, Yuchen
Nezhurina, Marianna
Berg-Kirkpatrick, Taylor
Dubnov, Shlomo",2024,arXiv:2211.06687 [cs],"Wu et al., “Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation.”",,http://arxiv.org/abs/2211.06687,,,,,2024-03-21,"Wu, Yusong, Ke Chen, Tianyu Zhang, Yuchen Hui, Marianna Nezhurina, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. “Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation.” arXiv, March 21, 2024. https://doi.org/10.48550/arXiv.2211.06687.",https://doi.org/10.48550/arXiv.2211.06687,https://zotero.org/users/local/bHD6JzYJ/items/CDSI3U7T,,/Users/slseanwu/Zotero/storage/FN6MZ2PA/Wu et al. - 2024 - Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmen.pdf,"Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.",67960-DL-project,"December 3, 2024 6:55 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"⁨Wu⁩ and ⁨Yang⁩, 2023",Compose & Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach,"Wu, Shih-Lun
Yang, Yi-Hsuan",2023,arXiv:2209.08212 [cs],"Wu and Yang, “Compose & Embellish.”",,http://arxiv.org/abs/2209.08212,,Compose & Embellish,,,2023-03-07,"Wu, Shih-Lun, and Yi-Hsuan Yang. “Compose & Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach.” arXiv, March 7, 2023. https://doi.org/10.48550/arXiv.2209.08212.",https://doi.org/10.48550/arXiv.2209.08212,https://zotero.org/users/local/bHD6JzYJ/items/ZBNWDQCC,,/Users/slseanwu/Zotero/storage/RKHKPM7L/Wu and Yang - 2023 - Compose & Embellish Well-Structured Piano Performance Generation via A Two-Stage Approach.pdf,"Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"Even with strong sequence models like Transformers, generating expressive piano performances with long-range musical structures remains challenging. Meanwhile, methods to compose well-structured melodies or lead sheets (melody + chords), i.e., simpler forms of music, gained more success. Observing the above, we devise a two-stage Transformer-based framework that Composes a lead sheet first, and then Embellishes it with accompaniment and expressive touches. Such a factorization also enables pretraining on non-piano data. Our objective and subjective experiments show that Compose & Embellish shrinks the gap in structureness between a current state of the art and real performances by half, and improves other musical aspects such as richness and coherence as well.",67960-DL-project,"December 3, 2024 6:56 PM (UTC)","December 3, 2024 6:56 PM (UTC)",,,Preprint,To Find,,
"⁨Loshchilov⁩ and ⁨Hutter⁩, 2019",Decoupled Weight Decay Regularization,"Loshchilov, Ilya
Hutter, Frank",2019,arXiv:1711.05101 [cs],"Loshchilov and Hutter, “Decoupled Weight Decay Regularization.”",,http://arxiv.org/abs/1711.05101,,,,,2019-01-04,"Loshchilov, Ilya, and Frank Hutter. “Decoupled Weight Decay Regularization.” arXiv, January 4, 2019. https://doi.org/10.48550/arXiv.1711.05101.",https://doi.org/10.48550/arXiv.1711.05101,https://zotero.org/users/local/bHD6JzYJ/items/LMSI8MDJ,,/Users/slseanwu/Zotero/storage/WK24DEUT/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf,"Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control",,,,,,,,,,,,"L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it ""weight decay"" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW",67960-DL-project,"December 4, 2024 6:31 PM (UTC)","December 4, 2024 6:31 PM (UTC)",,,Preprint,To Find,,
"Dao, 2023",FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,"Dao, Tri",2023,arXiv:2307.08691 [cs],"Dao, “FlashAttention-2.”",,http://arxiv.org/abs/2307.08691,,FlashAttention-2,,,2023-07-17,"Dao, Tri. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.” arXiv, July 17, 2023. https://doi.org/10.48550/arXiv.2307.08691.",https://doi.org/10.48550/arXiv.2307.08691,https://zotero.org/users/local/bHD6JzYJ/items/YDBJRYJT,,/Users/slseanwu/Zotero/storage/X2DLPT9Q/Dao - 2023 - FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning.pdf,Computer Science - Machine Learning,,,,,,,,,,,,"Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\times$ speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).",67960-DL-project,"December 4, 2024 7:27 PM (UTC)","December 4, 2024 7:28 PM (UTC)",,,Preprint,To Find,,
"Holtzman et al., 2020",The Curious Case of Neural Text Degeneration,"Holtzman, Ari
Buys, Jan
Du, Li
Forbes, Maxwell
Choi, Yejin",2020,arXiv:1904.09751 [cs],"Holtzman et al., “The Curious Case of Neural Text Degeneration.”",,http://arxiv.org/abs/1904.09751,,,,,2020-02-14,"Holtzman, Ari, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. “The Curious Case of Neural Text Degeneration.” arXiv, February 14, 2020. https://doi.org/10.48550/arXiv.1904.09751.",https://doi.org/10.48550/arXiv.1904.09751,https://zotero.org/users/local/bHD6JzYJ/items/SLLHWMYB,,/Users/slseanwu/Zotero/storage/998GZB4J/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf,Computer Science - Computation and Language,,,,,,,,,,,,"Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",67960-DL-project,"December 4, 2024 7:33 PM (UTC)","December 4, 2024 7:33 PM (UTC)",,,Preprint,To Find,,
"Huang et al., 2018",Music Transformer,"Huang, Cheng-Zhi Anna
Vaswani, Ashish
Uszkoreit, Jakob
Shazeer, Noam
Simon, Ian
Hawthorne, Curtis
Dai, Andrew M.
Hoffman, Matthew D.
Dinculescu, Monica
Eck, Douglas",2018,arXiv:1809.04281 [cs],"Huang et al., “Music Transformer.”",,http://arxiv.org/abs/1809.04281,,,,,2018-12-12,"Huang, Cheng-Zhi Anna, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. “Music Transformer.” arXiv, December 12, 2018. https://doi.org/10.48550/arXiv.1809.04281.",https://doi.org/10.48550/arXiv.1809.04281,https://zotero.org/sofronie/items/YQLG78FB,,C:\Users\pc\Zotero\storage\WV3IVBRM\Huang et al. - 2018 - Music Transformer.pdf,"Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning",,,,,,,,,,,,"Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",Notion_DL Music,"December 3, 2024 12:40 AM (UTC)","December 3, 2024 12:41 AM (UTC)",,,Preprint,To Find,,
"Dettmers et al., 2023",QLoRA: Efficient Finetuning of Quantized LLMs,"Dettmers, Tim
Pagnoni, Artidoro
Holtzman, Ari
Zettlemoyer, Luke",2023,arXiv:2305.14314 [cs],"Dettmers et al., “QLoRA.”",,http://arxiv.org/abs/2305.14314,,QLoRA,,,2023-05-23,"Dettmers, Tim, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. “QLoRA: Efficient Finetuning of Quantized LLMs.” arXiv, May 23, 2023. https://doi.org/10.48550/arXiv.2305.14314.",https://doi.org/10.48550/arXiv.2305.14314,https://zotero.org/users/local/bHD6JzYJ/items/TSAUTBAU,,/Users/slseanwu/Zotero/storage/CSJGBVQX/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf,Computer Science - Machine Learning,,,,,,,,,,,,"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",67960-DL-project,"December 7, 2024 2:27 PM (UTC)","December 7, 2024 2:27 PM (UTC)",,,Preprint,To Find,,
"Hu et al., 2021",LoRA: Low-Rank Adaptation of Large Language Models,"Hu, Edward J.
Shen, Yelong
Wallis, Phillip
Allen-Zhu, Zeyuan
Li, Yuanzhi
Wang, Shean
Wang, Lu
Chen, Weizhu",2021,arXiv:2106.09685 [cs],"Hu et al., “LoRA.”",,http://arxiv.org/abs/2106.09685,,LoRA,,,2021-10-16,"Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv, October 16, 2021. https://doi.org/10.48550/arXiv.2106.09685.",https://doi.org/10.48550/arXiv.2106.09685,https://zotero.org/users/local/bHD6JzYJ/items/3H6P7YX6,,/Users/slseanwu/Zotero/storage/QBVI5XGS/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,,,,,,,,,,,"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",67960-DL-project,"December 7, 2024 2:28 PM (UTC)","December 7, 2024 2:28 PM (UTC)",,,Preprint,To Find,,
"Gunasekar et al., 2023",Textbooks Are All You Need,"Gunasekar, Suriya
Zhang, Yi
Aneja, Jyoti
Mendes, Caio César Teodoro
Giorno, Allie Del
Gopi, Sivakanth
Javaheripi, Mojan
Kauffmann, Piero
Rosa, Gustavo de
Saarikivi, Olli
Salim, Adil
Shah, Shital
Behl, Harkirat Singh
Wang, Xin
Bubeck, Sébastien
Eldan, Ronen
Kalai, Adam Tauman
Lee, Yin Tat
Li, Yuanzhi",2023,arXiv:2306.11644 [cs],"Gunasekar et al., “Textbooks Are All You Need.”",,http://arxiv.org/abs/2306.11644,,,,,2023-10-02,"Gunasekar, Suriya, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, et al. “Textbooks Are All You Need.” arXiv, October 2, 2023. https://doi.org/10.48550/arXiv.2306.11644.",https://doi.org/10.48550/arXiv.2306.11644,https://zotero.org/users/local/bHD6JzYJ/items/YIZ7TM2D,,/Users/slseanwu/Zotero/storage/AJ5XH6RH/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,,,,,,,,,,,"We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality"" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",67960-DL-project,"December 7, 2024 2:29 PM (UTC)","December 7, 2024 2:29 PM (UTC)",,,Preprint,To Find,,
"Chu et al., 2024",Qwen2-Audio Technical Report,"Chu, Yunfei
Xu, Jin
Yang, Qian
Wei, Haojie
Wei, Xipin
Guo, Zhifang
Leng, Yichong
Lv, Yuanjun
He, Jinzheng
Lin, Junyang
Zhou, Chang
Zhou, Jingren",2024,arXiv:2407.10759 [eess],"Chu et al., “Qwen2-Audio Technical Report.”",,http://arxiv.org/abs/2407.10759,,,,,2024-07-15,"Chu, Yunfei, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, et al. “Qwen2-Audio Technical Report.” arXiv, July 15, 2024. https://doi.org/10.48550/arXiv.2407.10759.",https://doi.org/10.48550/arXiv.2407.10759,https://zotero.org/users/local/bHD6JzYJ/items/QZ32G77G,,/Users/slseanwu/Zotero/storage/SDMP9S2F/Chu et al. - 2024 - Qwen2-Audio Technical Report.pdf,"Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing",,,,,,,,,,,,"We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.",67960-DL-project,"December 7, 2024 2:30 PM (UTC)","December 7, 2024 2:30 PM (UTC)",,,Preprint,To Find,,
"Défossez et al., 2022",High Fidelity Neural Audio Compression,"Défossez, Alexandre
Copet, Jade
Synnaeve, Gabriel
Adi, Yossi",2022,arXiv:2210.13438 [eess],"Défossez et al., “High Fidelity Neural Audio Compression.”",,http://arxiv.org/abs/2210.13438,,,,,2022-10-24,"Défossez, Alexandre, Jade Copet, Gabriel Synnaeve, and Yossi Adi. “High Fidelity Neural Audio Compression.” arXiv, October 24, 2022. https://doi.org/10.48550/arXiv.2210.13438.",https://doi.org/10.48550/arXiv.2210.13438,https://zotero.org/users/local/bHD6JzYJ/items/462GIDJS,,/Users/slseanwu/Zotero/storage/D53C3H9V/Défossez et al. - 2022 - High Fidelity Neural Audio Compression.pdf,"Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning",,,,,,,,,,,,"We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.",67960-DL-project,"December 9, 2024 7:22 PM (UTC)","December 9, 2024 7:22 PM (UTC)",,,Preprint,To Find,,
"Choi et al., 2017",A Tutorial on Deep Learning for Music Information Retrieval,"Choi, Keunwoo
Fazekas, György
Cho, Kyunghyun
Sandler, Mark",2018,arXiv:1709.04396 [cs],"Choi et al., “A Tutorial on Deep Learning for Music Information Retrieval.”",,http://arxiv.org/abs/1709.04396,,,,,2018-05-03,"Choi, Keunwoo, György Fazekas, Kyunghyun Cho, and Mark Sandler. “A Tutorial on Deep Learning for Music Information Retrieval.” arXiv, May 3, 2018. https://doi.org/10.48550/arXiv.1709.04396.",https://doi.org/10.48550/arXiv.1709.04396,https://zotero.org/users/local/bHD6JzYJ/items/2EWFKC89,,/Users/slseanwu/Zotero/storage/84GTS2DU/Choi et al. - 2018 - A Tutorial on Deep Learning for Music Information Retrieval.pdf,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound",,,,,,,,,,,,"Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight. The goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for MIR. We lay out the basic principles and review prominent works in this hard to navigate the field. We then outline the network structures that have been successful in MIR problems and facilitate the selection of building blocks for the problems at hand. Finally, guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field.",67960-DL-project,"December 9, 2024 7:32 PM (UTC)","December 9, 2024 7:32 PM (UTC)",,,Preprint,To Find,,
"Wei et al., 2022",Finetuned Language Models Are Zero-Shot Learners,"Wei, Jason
Bosma, Maarten
Zhao, Vincent Y.
Guu, Kelvin
Yu, Adams Wei
Lester, Brian
Du, Nan
Dai, Andrew M.
Le, Quoc V.",2022,arXiv:2109.01652 [cs],"Wei et al., “Finetuned Language Models Are Zero-Shot Learners.”",,http://arxiv.org/abs/2109.01652,,,,,2022-02-08,"Wei, Jason, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. “Finetuned Language Models Are Zero-Shot Learners.” arXiv, February 8, 2022. https://doi.org/10.48550/arXiv.2109.01652.",https://doi.org/10.48550/arXiv.2109.01652,https://zotero.org/users/local/bHD6JzYJ/items/ER8RY26P,,/Users/slseanwu/Zotero/storage/XIHZTRLY/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf,Computer Science - Computation and Language,,,,,,,,,,,,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",67960-DL-project,"December 9, 2024 11:20 PM (UTC)","December 9, 2024 11:20 PM (UTC)",,,Preprint,To Find,,