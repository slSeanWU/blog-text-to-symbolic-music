<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>LLM-based Finetuning for Text to Symbolic Music Generation</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

.center-audio {
	display: flex;
	justify-content: center;
	align-items: center;
	margin: 0 auto;
	padding: 0 auto;
	/* margin: 0px;  Optional: Adds space above and below */
	/* padding: 0px; Optional: Adds space above and below */
}
audio {
	margin: 0 auto;
	padding: 0 auto;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="151a0c76-9fcd-8038-9f67-cabf1a296416" class="page serif"><header><div class="page-header-icon undefined"><span class="icon">🎼</span></div><h1 class="page-title">LLM-based Finetuning for Text to Symbolic Music Generation</h1><p class="page-description"></p></header><div class="page-body"><p id="151a0c76-9fcd-80c3-9874-c706751bbf1c" class=""> Shih-Lun Wu, Josephine Lee, and Sofronie Dun</p><p id="151a0c76-9fcd-80bb-8476-d3488f869a59" class=""><code>{slseanwu, jleey, sofronie}@mit.edu</code></p><p id="157a0c76-9fcd-8031-9aa4-ebaa4c87b072" class="">6.7960 (F24) Deep Learning Final Project</p><p id="158a0c76-9fcd-80e9-9f97-df170f2b2e42" class="">
</p><p id="154a0c76-9fcd-8039-8938-ff314e2250af" class="block-color-purple">💻 <strong>We open-source our implementations at </strong></p><p id="155a0c76-9fcd-80ac-b758-ddd4680819da" class=""><a href="https://github.com/slSeanWU/text-to-symbolic-music-LLM/tree/slseanwu-model">https://github.com/slSeanWU/text-to-symbolic-music-LLM/tree/slseanwu-model</a></p><h1 id="151a0c76-9fcd-80fa-95d2-c7a16634658e" class="">1. Introduction </h1><p id="154a0c76-9fcd-8059-8e0f-cbb06d976853" class="">Symbolic music generation models (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Huang%20et%20al%20,%202018%20155a0c769fcd81848954e03ff70f3183.html">Huang et al., 2018</a>; <a href="References%2014ea0c769fcd80198d65d16d264c3e46/%E2%81%A8Wu%E2%81%A9%20and%20%E2%81%A8Yang%E2%81%A9,%202023%20151a0c769fcd8133b27cf06af4a0b23f.html">⁨Wu⁩ and ⁨Yang⁩, 2023</a>; <a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a>) generate music as sequences of notes played by one or many instruments. Compared to the audio-domain counterparts (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Agostinelli%20et%20al%20,%202023%20151a0c769fcd813b8e9ff830924821e4.html">Agostinelli et al., 2023</a>; <a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a>), which produce continuous waveforms of multi-instrument mixtures, symbolic model outputs are more interpretable and editable, and hence can be more seamlessly integrated into musicians’ composition workflows.</p><p id="157a0c76-9fcd-80b3-9e82-c8cb5cf568b5" class="">However, an important conditioning modality, namely text prompting, is missing in current symbolic music generation models. This is in contrast to text-to-audio music models (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Agostinelli%20et%20al%20,%202023%20151a0c769fcd813b8e9ff830924821e4.html">Agostinelli et al., 2023</a>; <a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a>) due potentially to the relative scarcity of paired (text, symbolic music) data samples. The absence of text control means that users have to condition the model on individual notes and/or chords, which could be a slow and tedious process. Existing works that finetune (or repurpose) text LLMs for symbolic music tackled simple outputs like melody lines and chord progressions (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Yuan%20et%20al%20,%202024%20151a0c769fcd81248e6fdc80d28471dc.html">Yuan et al., 2024</a>; <a href="References%2014ea0c769fcd80198d65d16d264c3e46/Deng%20et%20al%20,%202024%20151a0c769fcd81d28c39c5c6cb5540a6.html">Deng et al., 2024</a>), which are musically much less rich and diverse than multi-instrument compositions.</p><p id="158a0c76-9fcd-80c5-847d-e426f01613a5" class="">Fortunately, a paired (text, symbolic music) dataset, MidiCaps (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Melechovsky%20et%20al%20,%202024%20151a0c769fcd8114ad24c0bc93f85ade.html">Melechovsky et al., 2024</a>), featuring 170K songs, was compiled recently—the authors leveraged LLMs to expand musical tags (e.g., keys, moods, genres, and chords) extracted from LMD dataset (<a href="https://colinraffel.com/publications/thesis.pdf">Raffel, 2016</a>) symbolic music samples to natural text descriptions. We believe that this new dataset provides a great avenue to explore methods to finetune either (unconditional) symbolic music models (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a>), or text LLMs (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Grattafiori%20et%20al%20,%202024%20151a0c769fcd818f87d3e3dc7f5fcb2f.html">Grattafiori et al., 2024</a>), which have no specialized musical skills but are trained on orders of magnitude larger data and hence boast impressive generalizability, to perform text-to-symbolic-music generation. In addition to comparing the two fine-tuning paradigms described above, we also propose several straightforward techniques hoping to enhance the inductive biases for both fine-tuning (or adaptation) paradigms.</p><h3 id="155a0c76-9fcd-8094-a5ae-e6bac3e386c4" class="">Contributions</h3><p id="155a0c76-9fcd-8093-8f19-e75cf395c6b1" class="">We propose and evaluate two main approaches for the task of text-conditioned multi-track symbolic music generation: </p><ol type="1" id="154a0c76-9fcd-800c-bc4d-e13a33749f51" class="numbered-list" start="1"><li><strong>Using Llama 3.2 LLM representations to enable text conditioning of pretrained unconditional symbolic music model</strong> <ol type="a" id="157a0c76-9fcd-8069-a1f4-d7f9def6cde9" class="numbered-list" start="1"><li>We integrate the Llama 3.2 text LLM (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Grattafiori%20et%20al%20,%202024%20151a0c769fcd818f87d3e3dc7f5fcb2f.html">Grattafiori et al., 2024</a>) with the Anticipatory Music Transformer (AMT) model (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a>) to enable text-conditioned multi-track symbolic music generation, using the (frozen) Llama text representations at the last layer as conditions to AMT.</li></ol><ol type="a" id="157a0c76-9fcd-80b1-994c-f8abf2e9af6c" class="numbered-list" start="2"><li>We propose a novel variant where the fine-tuned AMT model can learn weighted importances over Llama’s text representations at different layers, each of which may capture different levels of abstraction (e.g., semantics or high-level motifs). We expect that these layer-wise representation weights facilitate more nuanced mapping from text conditions to symbolic music outputs, and hence enhance text controllability.</li></ol></li></ol><p id="158a0c76-9fcd-8094-924b-d738c91e1422" class="">
</p><figure id="158a0c76-9fcd-80be-bc35-eec1eeff62a1" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/amt_model.png"><img style="width:3602px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/amt_model.png"/></a></figure><p id="157a0c76-9fcd-80a5-9b22-c244001dbfe1" class="block-color-blue_background"><em><mark class="highlight-orange"><mark class="highlight-blue_background">Figure 1</mark></mark></em><em><mark class="highlight-blue_background">: Enabling text control on a pretrained unconditional music generation model, namely, the Anticipatory Music Transformer (AMT), leveraging a frozen Llama 3.2 text LLM.</mark></em></p><p id="157a0c76-9fcd-808c-a13e-ca23873d5ac0" class="">
</p><ol type="1" id="154a0c76-9fcd-8066-b264-e51cf1668b6e" class="numbered-list" start="2"><li><strong>Finetuning Llama 3.2 text LLMs to repurpose them as a text to symbolic music generation model</strong><ol type="a" id="157a0c76-9fcd-8008-af29-cce97bfc79df" class="numbered-list" start="1"><li>We leverage AMT’s music tokenization scheme and expand the vocabulary (i.e., input embeddings and output projection) of Llama for it to take and generate music tokens.</li></ol><ol type="a" id="157a0c76-9fcd-8050-976d-cc06c0b0369b" class="numbered-list" start="2"><li>As an attempt to imbue Llama with better inductive biases on music, we additionally initialize expanded music vocabulary embeddings using pretrained embeddings from the AMT model. This approach leverages the <em>learned musical meanings</em><strong> </strong>of AMT embeddings while capitalizing on Llama’s generalization capabilities. We hope that  pretrained music embeddings help the model better capture the relationship between music tokens at the start of fine-tuning, and hence make it easier for the model to learn the nuances between text and music tokens.  </li></ol></li></ol><p id="158a0c76-9fcd-804c-9e0b-e3685cddf358" class="">
</p><figure id="158a0c76-9fcd-80a6-b71a-dda12053e012" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/llama_model.png"><img style="width:4221px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/llama_model.png"/></a></figure><p id="157a0c76-9fcd-800e-a490-f7147eef0f43" class="block-color-blue_background"><em><mark class="highlight-orange"><mark class="highlight-blue_background">Figure 2</mark></mark></em><em><mark class="highlight-blue_background">: Repurposing the Llama LLM for conditional symbolic music generation, by expanding its vocabulary to handle symbolic music tokens.</mark></em></p><h1 id="151a0c76-9fcd-8031-a57a-cf20540d9c5a" class="">2. Related Work  </h1><h2 id="151a0c76-9fcd-8099-be04-dc2bd4dd187e" class="">2.1. Text-to-music generation</h2><p id="154a0c76-9fcd-80ca-b9f6-ecdc61d43ab7" class="">Text-to-music generation involves creating music based on textual descriptions. For example, <em>MusicLM</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Agostinelli%20et%20al%20,%202023%20151a0c769fcd813b8e9ff830924821e4.html">Agostinelli et al., 2023</a>) leverages hierarchical sequence-to-sequence modeling to produce high-fidelity music conditioned on text descriptions. The model further supports melody-conditioned music generation, where a provided melody is transformed to match a specific stylistic description. Whereas <em>MusicGen </em>(<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a>) simplifies the generation process by introducing interleaving techniques to model multiple streams of compressed music representations based on the <em>EnCodec</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/De%CC%81fossez%20et%20al%20,%202022%20157a0c769fcd81dc9dc4cd7e1021dffa.html">Défossez et al., 2022</a>) audio tokenizer. This single-stage transformer-based approach enhances the efficiency of text-to-music generation and achieves competitive results in both human and automatic evaluations.</p><p id="154a0c76-9fcd-807e-bcc2-ce49e557eb13" class="">One critical drawback is their reliance on waveform outputs, which are inherently less editable. This restricts the ability to make fine-grained modifications to the generated music, a crucial feature for many music creators. Additionally, while MusicGen improves efficiency, it lacks fine-grained control over how closely the generated output adheres to the conditioning text or melody. These limitations emphasize the need to explore symbolic music generation approaches that provide more flexibility and control over musical elements such as harmony, rhythm, and texture.</p><h2 id="151a0c76-9fcd-809a-aa57-db5137637ff4" class="">2.2. Adapting LLMs to generate symbolic music</h2><p id="151a0c76-9fcd-8073-9adb-dda313efb6ba" class="">Symbolic music generation offers an alternative to waveform-based approaches by enabling the creation of editable representations, such as MIDI files. Models like <em>ChatMusician</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Yuan%20et%20al%20,%202024%20151a0c769fcd81248e6fdc80d28471dc.html">Yuan et al., 2024</a>) and <em>ComposerX </em>(<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Deng%20et%20al%20,%202024%20151a0c769fcd81d28c39c5c6cb5540a6.html">Deng et al., 2024</a>) adapted Large Language Models (LLMs) for symbolic music tasks by translating musical notes to text strings in, for example, ABC notation (<a href="https://en.wikipedia.org/wiki/ABC_notation">wiki</a>). These models perform well in generating coherent melodies and structured compositions (e.g., melodies paired with lyrics and/or chord progressions) using pretrained LLMs fine-tuned (or few-shot prompted) with textual music representations. However, their focus on relatively simple melodic structures falls short of capturing the richness and complexity typical of human-composed music. This gap underscores the importance of developing techniques that can model intricate harmonic progressions, rhythmic patterns, and dynamic textural details found in multi-instrument compositions.</p><h1 id="151a0c76-9fcd-8047-b295-c35ffad036e5" class="">3. Technical Background </h1><h2 id="151a0c76-9fcd-8010-92f5-e5c78a109dfb" class="">3.1. Tokenization scheme for multi-track symbolic music</h2><p id="154a0c76-9fcd-804b-8166-c96a73ec96b2" class="">We use the specialized tokenization scheme for multi-track symbolic music proposed in the Anticipatory Music Transformer (AMT) paper (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a>), encoding musical notes as context-free triplets: <code>(arrival time, instrument+pitch, note duration)</code> . That is, one note played by any instrument would become 3 tokens in the sequence.</p><p id="154a0c76-9fcd-80a4-a461-c194db9040cd" class="">We present a summary of the vocabulary below.</p><p id="154a0c76-9fcd-8012-be82-e1e4be34dceb" class="block-color-blue_background"><em><mark class="highlight-orange">Table 1</mark></em><em>: AMT’s token vocabulary for multitrack symbolic music</em></p><table id="154a0c76-9fcd-80b5-bb14-f0978790aad5" class="simple-table"><thead class="simple-table-header"><tr id="e00fd95e-cec6-4f65-b174-a1b5136b8938"><th id="^?Pi" class="simple-table-header-color simple-table-header" style="width:143px"><strong>Category</strong></th><th id="A\^L" class="simple-table-header-color simple-table-header" style="width:461px"><strong>Tokens</strong></th><th id="&lt;:]y" class="simple-table-header-color simple-table-header" style="width:103px"><strong>Vocab Size</strong></th></tr></thead><tbody><tr id="18508bdc-86c1-41dc-8371-77796be024f2"><th id="^?Pi" class="simple-table-header-color simple-table-header" style="width:143px">Arrival Time</th><td id="A\^L" class="" style="width:461px">Represents when a note begins; quantized time intervals (10 ms steps, maximum 100 seconds)</td><td id="&lt;:]y" class="" style="width:103px">10,000</td></tr><tr id="d9dac9e3-1877-4f7d-8e58-6ff567e98818"><th id="^?Pi" class="simple-table-header-color simple-table-header" style="width:143px">Instrument+Pitch</th><td id="A\^L" class="" style="width:461px">Combines the MIDI instrument identifier with the pitch of the note. There are 129 instruments and 128 possible pitches for each instrument.</td><td id="&lt;:]y" class="" style="width:103px">16,512</td></tr><tr id="154a0c76-9fcd-8044-adfc-cec5d449063c"><th id="^?Pi" class="simple-table-header-color simple-table-header" style="width:143px">Note duration</th><td id="A\^L" class="" style="width:461px">Quantized durations for each note (in 10ms steps, from 10ms to 10sec)</td><td id="&lt;:]y" class="" style="width:103px">1,000</td></tr><tr id="154a0c76-9fcd-8082-9b8d-f6d4b88bf099"><th id="^?Pi" class="simple-table-header-color simple-table-header" style="width:143px">Total</th><td id="A\^L" class="" style="width:461px">Unique tokens for all categories combined</td><td id="&lt;:]y" class="" style="width:103px">27,512</td></tr></tbody></table><h2 id="151a0c76-9fcd-8091-b8cb-c3fff4d63cfe" class="">3.2. Paired text descriptions for music</h2><p id="154a0c76-9fcd-8085-b300-f79617ca7457" class="">The MidiCaps dataset (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Melechovsky%20et%20al%20,%202024%20151a0c769fcd8114ad24c0bc93f85ade.html">Melechovsky et al., 2024</a>) combines symbolic music (MIDI files) with descriptive text captions, enabling the exploration of relationships between musical content and natural language. The text description for each music composition is generated following the process below.</p><ol type="1" id="154a0c76-9fcd-808d-9c2a-c914306260a2" class="numbered-list" start="1"><li><strong>Music Feature Extraction: </strong>The pipeline extracts several music-specific features from MIDI files using music information retrieval (MIR) tools (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Choi%20et%20al%20,%202017%20157a0c769fcd819c9e06d43fa9e68a79.html">Choi et al., 2017</a>). These features include tempo (the pace of the piece), chord progression, time signature, instrument presence, genre, and mood.</li></ol><ol type="1" id="154a0c76-9fcd-805a-99e4-f35bc6692a4f" class="numbered-list" start="2"><li><strong>Transformation into Natural Language</strong>: After extracting these features, the pipeline uses a large language model (LLM), specifically Anthropic&#x27;s Claude 3, to generate descriptive captions. By leveraging in-context learning, the LLM transforms the extracted symbolic features into fluent natural language sentences, capturing the music&#x27;s essence in human-readable form. This approach effectively bridges MIR and natural language processing, creating a versatile dataset for tasks like text-to-MIDI generation and music captioning.</li></ol><h1 id="151a0c76-9fcd-8047-af68-f7bb89b0e05c" class="">4. Method</h1><h2 id="151a0c76-9fcd-80b1-b0cd-f397b2f3c047" class="">4.1. Starting from (unconditional) music model (AMT) </h2><p id="154a0c76-9fcd-80bf-afa5-f83ac8bf03ec" class="">We add the following network components to the existing music model, namely, the AMT model (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a>):</p><ul id="154a0c76-9fcd-8026-82ee-cbaa49e51f24" class="bulleted-list"><li style="list-style-type:disc"><strong>Llama 3.2 (1B) model integration: </strong>A pre-trained Llama 3.2 (1B) model is loaded and frozen to provide rich contextual embeddings for text conditioning. The weights are frozen and there is a specialized padding token to handle batched training and inference. A learnable linear layer projects the Llama hidden states into the AMT embedding space.</li></ul><ul id="154a0c76-9fcd-80f0-984b-c3c2071f1ab2" class="bulleted-list"><li style="list-style-type:disc"><strong>Concatenation with music tokens</strong>: The projected Llama text embeddings are prepended, along the timestep dimension, to the AMT token embeddings before entering the AMT backbone. Additionally, We ensure that the Llama text embeddings, which already encode positional information inside Llama, do not receive AMT’s positional embeddings to avoid redundant (and potentially misleading) positional information.</li></ul><p id="154a0c76-9fcd-8067-bd74-ea082aa6c45f" class="">We propose two versions of AMT models:</p><ul id="154a0c76-9fcd-80ce-9ee9-d5fbdceb4399" class="bulleted-list"><li style="list-style-type:disc"><strong>Base Version</strong>: The base model directly uses the final hidden state from Llama as the conditioning input to AMT, following MusicGen (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a>).</li></ul><ul id="154a0c76-9fcd-80d3-a874-ecd5108b36c1" class="bulleted-list"><li style="list-style-type:disc"><strong>Enhanced Version:</strong> All Llama hidden states (from 16 Transformer layers and the embedding layer) are stacked into a tensor and combined as a weighted sum. The weights are learnable parameters, enabling the model to learn and decide the importance of individual layers dynamically during training.</li></ul><p id="154a0c76-9fcd-807e-b2f1-dd77bfc2fde0" class="">The loss function for next-token prediction is defined as the negative log-likelihood of the target token given the input sequence. We denote <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span><span>﻿</span></span> as the music tokens, <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span><span>﻿</span></span> as the text tokens, and <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span></span><span>﻿</span></span> as the music token sequence length. </p><figure id="154a0c76-9fcd-8034-987e-f200e6dc49dc" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo separator="true">,</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">-\log p(x | y) = -\sum_{t=1}^{T} \log p(x_t | y, x_{&lt;t})\, .</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">.</span></span></span></span></span></div></figure><p id="157a0c76-9fcd-80a8-a7ef-ecc5beda933d" class="">At the beginning of all music tokens, we place a <code>[start of music]</code> token to signal the model that text description has ended and that it should start generating music.</p><h2 id="151a0c76-9fcd-809a-9bc9-c4ae9e79238a" class="">4.2. Starting from text model (Llama 3.2)</h2><p id="154a0c76-9fcd-808f-b10c-e5ab2df98b45" class="">We experiment with both the 1B and 3B version of Llama 3.2 and add the following network components to the existing Llama text model to enable it to generate music.</p><ul id="154a0c76-9fcd-800a-b73c-f9152c7546e5" class="bulleted-list"><li style="list-style-type:disc"><strong>Expanded Vocabulary Handling: </strong>We add an embedding layer for the extended music vocabulary and a linear projection layer to align the embeddings of the extended vocabulary with the pretrained representation space of Llama. The extended vocabulary is exactly the same as that of AMT.</li></ul><ul id="157a0c76-9fcd-80c6-86e8-f05efc5d3e1d" class="bulleted-list"><li style="list-style-type:disc"><strong>Prefix Instruction: </strong>Inspired by instruction fine-tuning (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Wei%20et%20al%20,%202022%20157a0c769fcd8116b338e927a0481d94.html">Wei et al., 2022</a>), which aligns model’s behavior with user input specification, we prepend to the text description a short instruction <code>You are a world-class composer. Please compose some music according to the following description:</code> to<em> nudge</em> the LLM into a musician’s mindset.</li></ul><p id="154a0c76-9fcd-808a-ac68-f7da3b1f43e9" class="">We propose two types of initialization:</p><ul id="154a0c76-9fcd-8057-889c-f4e57b9e8da6" class="bulleted-list"><li style="list-style-type:disc"><strong>Base Version:</strong> the extended vocabulary embeddings are initialized following the Llama model’s standard initialization method, i.e., gaussian random initialization.</li></ul><ul id="154a0c76-9fcd-806d-834d-de157853ed0b" class="bulleted-list"><li style="list-style-type:disc"><strong>Enhanced Version: </strong>For the extended vocabulary embeddings, weights are initialized by copying from a pretrained AMT&#x27;s token embeddings, and scaling them such that the standard deviation of the embeddings match that of the Llama’s pretrained text token embeddings, and finally, zeros are filled for the dimensions non-existent in AMT (AMT has token embedding dimension of 1280, while Llama has either 2048 or 3072, depending on the model size).</li></ul><p id="157a0c76-9fcd-80be-8cf2-df30a3a35636" class="">For tokens in the original vocabulary, the model predicts based on <code>lm_head</code>, which maps hidden states to logits corresponding to the original Llama (text) vocabulary. For tokens in the extended vocabulary, <code>expanded_lm_head</code> produces logits corresponding to the extended music vocabulary. These logits are finally concatenated with those from the original vocabulary to produce a single output distribution over all text and music tokens. </p><p id="158a0c76-9fcd-80fe-b177-e390d0f4160c" class="">Unlike in AMT, we apply cross-entropy loss over both the text and music vocabularies as its loss function, written as:</p><figure id="157a0c76-9fcd-804e-861f-ec1d7659a207" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo stretchy="false">(</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo separator="true">,</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><munderover><mo>∑</mo><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi></mrow></msub></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mo>&lt;</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">-\log p(x, y) = -(\sum_{t=1}^{T} \log p(x_t | y, x_{&lt;t}) + \sum_ {t&#x27;=1}^{T_{\mathrm{text}}} \log p(y_{t&#x27;} | y_{&lt;t&#x27;})) \, ,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord">−</span><span class="mopen">(</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:3.1334em;vertical-align:-1.294em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8394em;"><span style="top:-1.856em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3111em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.294em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span></span></span></span></span></div></figure><p id="157a0c76-9fcd-8070-b9b7-ffddd000c6de" class="">where <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{\mathrm{text}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> is the number of tokens in the text prompt. The <code>[start of music]</code> token is placed the same way as in our AMT models.</p><h1 id="151a0c76-9fcd-80b7-999b-eb2578ee0e19" class="">5. Experiments</h1><h2 id="151a0c76-9fcd-80c5-a6a2-f5879c79f7b1" class="">5.1. Implementation and hyperparameters</h2><p id="152a0c76-9fcd-80a4-a56c-f4f6c8bf25b9" class="block-color-blue_background"><em><mark class="highlight-orange">Table 2</mark></em><em>: Model / training / inference hyperparameters</em></p><table id="152a0c76-9fcd-8053-b306-c957c0c693e6" class="simple-table"><thead class="simple-table-header"><tr id="a3345d8c-fa73-4b3a-8c76-0a0d86240e2c"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Attribute \ Model</th><th id="qHBd" class="simple-table-header-color simple-table-header" style="width:172.5px"><strong>AMT Large</strong></th><th id="erhs" class="simple-table-header-color simple-table-header" style="width:172.5px"><strong>Llama 3.2 (1B)</strong></th><th id="F=aZ" class="simple-table-header-color simple-table-header" style="width:172.5px"><strong>Llama 3.2 (3B)</strong></th></tr></thead><tbody><tr id="b8993de3-62ef-4e42-a8a4-604a216b2ba9"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Trainable params</th><td id="qHBd" class="" style="width:172.5px">783 M</td><td id="erhs" class="" style="width:172.5px">1.47 B</td><td id="F=aZ" class="" style="width:172.5px">3.56 B</td></tr><tr id="d6ab8092-60ff-438d-a4d6-c74d6a6076e4"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Optimizer</th><td id="qHBd" class="" style="width:172.5px">AdamW</td><td id="erhs" class="" style="width:172.5px">AdamW</td><td id="F=aZ" class="" style="width:172.5px">AdamW</td></tr><tr id="152a0c76-9fcd-8095-91ae-e3c76fcc8f0e"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Training seq length</th><td id="qHBd" class="" style="width:172.5px">1024</td><td id="erhs" class="" style="width:172.5px">1024</td><td id="F=aZ" class="" style="width:172.5px">1024</td></tr><tr id="152a0c76-9fcd-8065-baf4-d449594f1b3d"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Training batch size</th><td id="qHBd" class="" style="width:172.5px">16</td><td id="erhs" class="" style="width:172.5px">64</td><td id="F=aZ" class="" style="width:172.5px">64</td></tr><tr id="152a0c76-9fcd-8018-9b0d-fd27d38bbae6"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Peak learning rate</th><td id="qHBd" class="" style="width:172.5px">1e-5</td><td id="erhs" class="" style="width:172.5px">2e-4</td><td id="F=aZ" class="" style="width:172.5px">2e-4</td></tr><tr id="152a0c76-9fcd-808e-8cf8-ca33cf9da785"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Max training steps</th><td id="qHBd" class="" style="width:172.5px">60 K</td><td id="erhs" class="" style="width:172.5px">15 K</td><td id="F=aZ" class="" style="width:172.5px">15 K</td></tr><tr id="152a0c76-9fcd-8027-bea1-db8cb405ce3f"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Warmup (linear) steps</th><td id="qHBd" class="" style="width:172.5px">500</td><td id="erhs" class="" style="width:172.5px">500</td><td id="F=aZ" class="" style="width:172.5px">500</td></tr><tr id="152a0c76-9fcd-8031-95f4-c956be8ac6ca"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Learning rate schedule</th><td id="qHBd" class="" style="width:172.5px">cosine</td><td id="erhs" class="" style="width:172.5px">cosine</td><td id="F=aZ" class="" style="width:172.5px">cosine</td></tr><tr id="154a0c76-9fcd-80cc-8e37-e8fca44b4f8d"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Precision</th><td id="qHBd" class="" style="width:172.5px">bf16-mixed</td><td id="erhs" class="" style="width:172.5px">bf16-mixed</td><td id="F=aZ" class="" style="width:172.5px">bf16-mixed</td></tr><tr id="152a0c76-9fcd-80e0-a32e-dde5857e2d89"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Inference temperature</th><td id="qHBd" class="" style="width:172.5px">1.0</td><td id="erhs" class="" style="width:172.5px">1.0 / 1.1</td><td id="F=aZ" class="" style="width:172.5px">1.0 / 1.1</td></tr><tr id="152a0c76-9fcd-8051-8d54-efc1246e4b16"><th id="{eZJ" class="simple-table-header-color simple-table-header" style="width:172.5px">Inference top-p</th><td id="qHBd" class="" style="width:172.5px">0.98</td><td id="erhs" class="" style="width:172.5px">0.98</td><td id="F=aZ" class="" style="width:172.5px">0.98</td></tr></tbody></table><h3 id="152a0c76-9fcd-805e-a0f5-ce1456e687eb" class="">Training</h3><p id="152a0c76-9fcd-807f-9a1e-d2b11e735501" class="">We implement our changes based on HuggingFace <code><a href="https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a></code> (for AMT) and <code><a href="https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaForCausalLM">LlamaForCausalLM</a></code> (for Llama) model classes, and leverage HuggingFace <code>Trainer</code> to train models.</p><p id="151a0c76-9fcd-8032-993f-f0e5b5d7f247" class="">Following both AMT and Llama papers, we use the <em>AdamW</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/%E2%81%A8Loshchilov%E2%81%A9%20and%20%E2%81%A8Hutter%E2%81%A9,%202019%20152a0c769fcd815e804feab17736b06c.html">⁨Loshchilov⁩ and ⁨Hutter⁩, 2019</a>) optimizer for training. Following AMT’s pretraining setup, we restrict our training sequence length to 1024 tokens, which equates to about 15 seconds of music (but highly variable due to varying note density). For the key training hyperparameters, we perform a small-scale search across:</p><ul id="152a0c76-9fcd-8009-88e9-c5931719d19d" class="bulleted-list"><li style="list-style-type:disc"><em>batch size</em> = {16, 64}</li></ul><ul id="152a0c76-9fcd-8091-9164-dc36956e2ab7" class="bulleted-list"><li style="list-style-type:disc"><em>learning rate</em> = {1e-5, 2e-4},</li></ul><p id="152a0c76-9fcd-801b-9a89-d9858444bda2" class="">and find that AMT trains better with <em>small batch size </em>(i.e., 16) and <em>low learning rate</em> (i.e., 1e-5), while Llama, in contrast, favors the exact opposite. Maximum training (i.e., gradient update) steps is set to 60K and 15K respectively for AMT and Llama so that the models would see around 1B tokens in total, roughly equal to the token count of our training dataset (but is not 1 epoch, to be explained in Section 5.2). A summary of the hyperparameters can be found in <mark class="highlight-orange">Table 2</mark>.</p><p id="152a0c76-9fcd-80f7-a32a-e5eb640013f1" class="">We train each of the models on a single NVidia A100 GPU with 80GB of graphics memory, and we accumulate the gradients until reaching the specified batch sizes above. It takes around 1 day to train the Llama 3.2 (1B) model, and roughly 2 days for the Llama 3.2 (3B) and AMT Large models. The AMT Large model trains slower, despite its smaller size, due to not having a valid <em>FlashAttention 2 </em>(<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Dao,%202023%20152a0c769fcd81308c68fe5c1a5c8724.html">Dao, 2023</a>) implementation.   </p><h3 id="152a0c76-9fcd-805a-bb6e-e73764bac454" class="">Inference</h3><p id="152a0c76-9fcd-8062-8d87-f576ed9ae76c" class="">To generate samples for evaluation, we condition the models with a text prompt and ask them to generate 1024 music tokens, staying consistent with how the models are trained. For Llama models, we utilize HuggingFace’s <code>model.generate()</code> method. For the AMT model, due to <code>model.generate()</code> not being compatible with additional text conditioning, we implement our own generation loop.</p><p id="152a0c76-9fcd-80fc-9260-ecd645aff450" class="">Following AMT’s inference setup, we use <em>nucleus sampling</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Holtzman%20et%20al%20,%202020%20152a0c769fcd810babcbf934793b7b22.html">Holtzman et al., 2020</a>) with a <em>top-p</em> hyperparameter of 0.98. However, for Llama models, this setting sometimes leads to degenerate samples, e.g., unreasonably many notes at the same arrival time, and the arrival time never advances — we find that applying a higher temperature, i.e., 1.1 instead of the default 1.0, largely resolves the issue with the tradeoff that the outputs sound more chaotic.</p><p id="152a0c76-9fcd-80d1-85e3-c8672470633e" class="">With batched generation, generating 1K samples takes around <strong>30 / 40 / 60</strong> minutes for the <strong>Llama (1B) / Llama (3B) / AMT Large</strong><em><strong> </strong></em>models respectively, also on a single A100 (80G) GPU.</p><p id="152a0c76-9fcd-807b-be0d-ee336564a54d" class="">As our evaluation metrics take audio inputs (and it’s also easier for humans to judge the outputs with audios rather than a sequence of tokens), we utilize the <em><a href="https://www.fluidsynth.org/api/index.html">FluidSynth</a></em> to synthesize the music tokens into audio waveforms.</p><h2 id="152a0c76-9fcd-804c-9891-dae939bf94f2" class="">5.2. Dataset and data processing</h2><p id="152a0c76-9fcd-8034-803b-f8948ba1feae" class="">We perform an intersection between the valid MIDI files in the <em>LMD</em> dataset (<strong><a href="https://colinraffel.com/projects/lmd/">Raffel, 2016</a></strong>), and those which are paired with a text description in the <em>MidiCaps</em> dataset (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Melechovsky%20et%20al%20,%202024%20151a0c769fcd8114ad24c0bc93f85ade.html">Melechovsky et al., 2024</a>), available on HuggingFace <code><a href="https://huggingface.co/datasets/amaai-lab/MidiCaps">datasets</a></code>, to construct our paired (text, music) dataset. This step leaves us with 151,526 samples (i.e., songs) in total.</p><p id="152a0c76-9fcd-80e8-b322-ea0810399578" class="">Following the splitting strategy described in the AMT paper (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a>), we split the samples to:</p><ul id="152a0c76-9fcd-8054-9cfb-cd9365b9d5e4" class="bulleted-list"><li style="list-style-type:disc"><code>train</code> set — LMD hex ID starting with <code>[0-9|a-d]</code> , 132,460 samples</li></ul><ul id="152a0c76-9fcd-80a8-a700-d5b941844e2a" class="bulleted-list"><li style="list-style-type:disc"><code>valid</code> set — LMD hex ID starting with <code>e</code>, 9,618 samples</li></ul><ul id="152a0c76-9fcd-805a-b3d2-d149363638aa" class="bulleted-list"><li style="list-style-type:disc"><code>test</code> set — LMD hex ID starting with <code>f</code>, 9,448 samples</li></ul><p id="152a0c76-9fcd-8027-9977-ce2ad59efc49" class="">Considering limited time and compute, we use only the first <strong>4K </strong>and <strong>1K</strong> samples in <code>valid</code> and <code>test</code> sets respectively. Since the samples have vastly different duration, and hence the token sequence length, we divide each sample into non-overlapping chunks of 1,024 tokens (i.e., the same as our training sequence length), which gives us around 1B tokens in total.</p><p id="152a0c76-9fcd-8029-ab0c-f04faea1effb" class="">Since there is only one text description per sample (i.e., multiple chunks in the same sample correspond to the same text description), when looping through the dataset, we treat one epoch as looping through all the text descriptions. That is, if one sample has four chunks, the <code>[1st, 2nd, 3rd, 4th, 1st, …]</code> chunks get fed to the model in the <code>[1st, 2nd, 3rd, 4th, 5th, ...]</code> epoch. Given our maximum training steps (15K with batch size 64, or, 60K with batch size 16), we are training our models for around 7 epochs.</p><h2 id="151a0c76-9fcd-8040-8ca5-e545c4487082" class="">5.3. Evaluation metrics </h2><p id="152a0c76-9fcd-8073-8db0-f44849770c65" class="">We follow conventions from recent text-to-music research (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Agostinelli%20et%20al%20,%202023%20151a0c769fcd813b8e9ff830924821e4.html">Agostinelli et al., 2023</a>, <a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a>) and adopt the two metrics below, measuring <strong>quality</strong> and <strong>text controllability</strong> respectively.</p><h3 id="152a0c76-9fcd-80e7-830f-cf25fca0ae7b" class=""><strong>Fréchet audio distance</strong> (FAD) </h3><p id="157a0c76-9fcd-8094-a1bc-f508f550bc33" class="">FAD evaluates the <strong>quality</strong> (or, precisely, the <strong>realisticness</strong> compared to ground-truth human compositions) of the set of all generated samples as a whole (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Kilgour%20et%20al%20,%202019%20151a0c769fcd810ab31ff2cfca3a1624.html">Kilgour et al., 2019</a>). It leverages a pretrained audio feature extractor (e.g., an audio event classifier) to obtain embeddings from each generated (and ground truth) sample, estimate the gaussian distribution parameters (i.e., means and covariances) from the set of all generated/ground-truth audio embeddings, and compute the Fréchet distance between two gaussian distributions, i.e.,</p><figure id="153a0c76-9fcd-8077-a8c6-f4066d72f8e4" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">F</mi><mrow><mo fence="true">(</mo><msub><mi mathvariant="script">N</mi><mi>b</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="script">N</mi><mi>e</mi></msub><mo fence="true">)</mo></mrow><mo>=</mo><msup><mrow><mo fence="true">∥</mo><msub><mi>μ</mi><mi>b</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>e</mi></msub><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mo>+</mo><mi mathvariant="normal">tr</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="normal">Σ</mi><mi>b</mi></msub><mo>+</mo><msub><mi mathvariant="normal">Σ</mi><mi>e</mi></msub><mo>−</mo><mn>2</mn><msqrt><mrow><msub><mi mathvariant="normal">Σ</mi><mi>b</mi></msub><msub><mi mathvariant="normal">Σ</mi><mi>e</mi></msub></mrow></msqrt><mo fence="true">)</mo></mrow><mtext> </mtext><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathbf{F}\left(\mathcal{N}_b, \mathcal{N}_e\right)=\left\|\mu_b-\mu_e\right\|^2+\operatorname{tr}\left(\Sigma_b+\Sigma_e-2 \sqrt{\Sigma_b \Sigma_e}\right) \, ,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbf">F</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1474em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1474em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.204em;vertical-align:-0.25em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mop"><span class="mord mathrm">tr</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">2</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0005em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9605em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2395em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span></span></span></span></span></div></figure><p id="153a0c76-9fcd-80b4-b543-d004369dc4b4" class="">where <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">N</mi><mi>e</mi></msub><mrow><mo fence="true">(</mo><msub><mi>μ</mi><mi>e</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="normal">Σ</mi><mi>e</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}_e\left(\mu_e, \Sigma_e\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1474em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span><span>﻿</span></span> are estimated gaussian distribution from the <em>generated</em> set embeddings, and <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">N</mi><mi>b</mi></msub><mrow><mo fence="true">(</mo><msub><mi>μ</mi><mi>b</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="normal">Σ</mi><mi>b</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}_b\left(\mu_b, \Sigma_b\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1474em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span><span>﻿</span></span> are that from the <em>ground-truth</em> set embeddings. </p><h3 id="152a0c76-9fcd-806a-96cc-f513a8e4c28a" class=""><strong>CLAP score</strong> </h3><p id="157a0c76-9fcd-8075-b8a0-ed602cbf96d6" class="">CLAP score measures the <strong>relevance </strong>of the generated music <strong>w.r.t. the input text description</strong>. It<strong> </strong>utilizes contrastively pretrained dual audio and text encoders, i.e., CLAP (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Wu%20et%20al%20,%202023%20151a0c769fcd81d09d65e353e5a3453d.html">Wu et al., 2023</a>), to extract the text embedding (of an input text description) and the audio embedding (of the generated music). Then, it computes the cosine similarity between text and audio embeddings:</p><figure id="154a0c76-9fcd-80c8-a47f-e38965ada809" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">P</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi></mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>a</mi></msub><mo separator="true">,</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>E</mi><mi>a</mi></msub><mo separator="true">,</mo><msub><mi>E</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{CLAPScore}(X_a, X_t) = \cos(E_a, E_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">CLAPScore</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><p id="151a0c76-9fcd-8080-b645-ffc8f576ac62" class="">where <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">X_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> is the audio (synthesized from the generated music), <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> is the input text description, <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>a</mi></msub><mo>=</mo><msub><mtext>CLAP</mtext><mtext>audio</mtext></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mi>a</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_a = \text{CLAP}_{\text{audio}}(X_a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">CLAP</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">audio</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> is the audio embedding, which is the CLAP audio encoder features, normalized to be on a unit hypersphere, and <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>t</mi></msub><mo>=</mo><msub><mtext>CLAP</mtext><mtext>text</mtext></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_t = \text{CLAP}_{\text{text}}(X_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">CLAP</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> is the text embedding, i.e., CLAP text encoder features, also normalized to be on a unit hypersphere.</p><h1 id="151a0c76-9fcd-8032-b4b5-df75f7b14d62" class="">6. Results and Discussions</h1><h2 id="151a0c76-9fcd-80e9-a710-de098cc31f7c" class="">6.1. Quantitative results </h2><h3 id="154a0c76-9fcd-80bb-9c29-ef8f280fdd69" class="">Training and validation losses</h3><figure id="154a0c76-9fcd-80b0-a53a-cc45d833fe94" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/loss_plot.png"><img style="width:708px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/loss_plot.png"/></a></figure><p id="154a0c76-9fcd-80d8-a33f-e73b993f5d25" class="block-color-blue_background"><em><mark class="highlight-orange">Figure 3</mark></em><em>: Training and validation losses of our models. Note that the losses of AMT and Llama models are not directly comparable since the latter models both text and music tokens.</em></p><p id="154a0c76-9fcd-8092-bc96-faf3c618c462" class="">
</p><p id="154a0c76-9fcd-80a0-879c-f7565b681055" class="">We first examine how the training process goes for each model by observing the losses, which are plotted in <mark class="highlight-orange">Figure 3</mark>.</p><p id="154a0c76-9fcd-8007-b46e-f59fd70af14b" class="">For the <strong>AMT-Large</strong> models, both <em>with</em> or <em>without</em> weighted text representations from the Llama 3.2 (1B) LLM, validation losses converge very quickly within the first 5K steps. Training losses continue to decrease slowly throughout the 60K steps, but there is no sign of serious overfitting. The quick convergence of validation loss potentially suggests the pretrained AMT backbone is not really trying to make use of the text conditioning signals, and still mostly relies on the music context, which it has extensively seen during pretraining, to predict the next tokens. The variant with weighted Llama states does score a slightly lower (better) validation loss. We take a look at the trained weights (to combine text representations of all Llama layers) and observe that the model assigns an almost uniform weights to all layers, indicating that the model finds a simple combination of all layers’ representations can be helpful in optimizing the loss, instead of only using the last layer as done in prior works (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a>, <a href="References%2014ea0c769fcd80198d65d16d264c3e46/Evans%20et%20al%20,%202024%20151a0c769fcd8168bdd6d97d8467939a.html">Evans et al., 2024</a>).</p><p id="154a0c76-9fcd-80e6-a946-ccd916aa1970" class="">The training dynamics for the <strong>Llama 3.2 LLMs</strong>, in contrast, is clearly distinct from that of AMT. Both 1B and 3B versions are able to easily overfit our training dataset (with an order 100K text and symbolic music pairs), and the validation loss starts to worsen at only around 6K steps, which is around 3 epochs over the training set. We take this as a good sign as it indicates that text LLMs generally have no difficulty fitting music data, even though the entire music token vocabulary is new. Besides, our AMT embedding initialization technique, which we experiment with the 3B version, does not have a significant effect on the training and validation losses.</p><h3 id="154a0c76-9fcd-80b8-9fb2-e2148db14cd7" class="">Evaluation of generated music</h3><p id="154a0c76-9fcd-808a-9412-f55324f792b9" class="block-color-blue_background"><em><mark class="highlight-orange">Table 3</mark></em><em>: Evaluation results on 1K generated samples, conditioned on the first 1K descriptions of our test set</em>.<em> To quantify FAD’s uncertainty, we compute </em><em><strong>FAD-sub</strong></em><em> by randomly drawing 5 sets of 500 samples from the generated 1K samples and compute FAD for each set. Standard error of mean (SEM) follows </em><em><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span></em><em>.</em></p><table id="154a0c76-9fcd-8075-ae9a-c108b02f9c87" class="simple-table"><thead class="simple-table-header"><tr id="154a0c76-9fcd-80a6-9e38-c27c91cfe010"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px">Model \ Metrics</th><th id="Id|b" class="simple-table-header-color simple-table-header" style="width:115px"><strong>Temperature</strong></th><th id="Lt\b" class="simple-table-header-color simple-table-header" style="width:115px"><strong>Val loss (</strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↓</span></span></span></span></span><span>﻿</span></span><strong>)</strong></th><th id="zuMO" class="simple-table-header-color simple-table-header" style="width:115px"><strong>FAD (</strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↓</span></span></span></span></span><span>﻿</span></span><strong>)</strong></th><th id="kqU`" class="simple-table-header-color simple-table-header" style="width:115px"><strong>FAD-sub (</strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↓</span></span></span></span></span><span>﻿</span></span><strong>)</strong></th><th id="K~q@" class="simple-table-header-color simple-table-header" style="width:115px"><strong>CLAP </strong>(<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↑</mo></mrow><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↑</span></span></span></span></span><span>﻿</span></span>)</th></tr></thead><tbody><tr id="154a0c76-9fcd-802d-85d4-d515ff6bd2aa"><th id="hmyS" class="block-color-gray_background simple-table-header" style="width:132px"><em>Baselines</em></th><td id="Id|b" class="block-color-gray_background" style="width:115px"></td><td id="Lt\b" class="block-color-gray_background" style="width:115px"></td><td id="zuMO" class="block-color-gray_background" style="width:115px"></td><td id="kqU`" class="block-color-gray_background" style="width:115px"></td><td id="K~q@" class="block-color-gray_background" style="width:115px"></td></tr><tr id="154a0c76-9fcd-8044-9108-e8fd247af653"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px">    <strong>Ground truth</strong></th><td id="Id|b" class="" style="width:115px">—</td><td id="Lt\b" class="" style="width:115px">—</td><td id="zuMO" class="" style="width:115px">—</td><td id="kqU`" class="" style="width:115px">—</td><td id="K~q@" class="" style="width:115px">0.238 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-800c-af57-e9f27b88de4d"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px">    <strong>Ground truth <br/>    (random text)<br/></strong></th><td id="Id|b" class="" style="width:115px">—</td><td id="Lt\b" class="" style="width:115px">—</td><td id="zuMO" class="" style="width:115px">—</td><td id="kqU`" class="" style="width:115px">—</td><td id="K~q@" class="" style="width:115px">0.158 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003 </td></tr><tr id="154a0c76-9fcd-80cb-8cc3-f0f501f5a3e1"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px">    <strong>AMT-Large <br/>    (unconditional)<br/></strong></th><td id="Id|b" class="" style="width:115px">1.0</td><td id="Lt\b" class="" style="width:115px">—</td><td id="zuMO" class="" style="width:115px">0.781</td><td id="kqU`" class="" style="width:115px">0.833 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.025</td><td id="K~q@" class="" style="width:115px">0.120 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-80e9-ab86-d8bf065b9891"><th id="hmyS" class="block-color-gray_background simple-table-header" style="width:132px"><em>AMT-Large <br/>+ text cond<br/></em></th><td id="Id|b" class="block-color-gray_background" style="width:115px"></td><td id="Lt\b" class="block-color-gray_background" style="width:115px"></td><td id="zuMO" class="block-color-gray_background" style="width:115px"></td><td id="kqU`" class="block-color-gray_background" style="width:115px"></td><td id="K~q@" class="block-color-gray_background" style="width:115px"></td></tr><tr id="154a0c76-9fcd-8049-94fc-f17d066f52c5"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px">    <strong>w/o weighted <br/>    Llama states<br/></strong></th><td id="Id|b" class="" style="width:115px">1.0</td><td id="Lt\b" class="" style="width:115px">0.642</td><td id="zuMO" class="" style="width:115px">0.299</td><td id="kqU`" class="" style="width:115px">0.332 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.035</td><td id="K~q@" class="" style="width:115px">0.156 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-80a3-af48-c0f0d1f0f3f8"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px"><strong>    w/ weighted <br/>    Llama states<br/></strong></th><td id="Id|b" class="" style="width:115px">1.0</td><td id="Lt\b" class="" style="width:115px">0.637</td><td id="zuMO" class="" style="width:115px">0.302</td><td id="kqU`" class="" style="width:115px">0.334 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.021</td><td id="K~q@" class="" style="width:115px">0.153 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-80d5-8967-eb240782e54e"><th id="hmyS" class="block-color-gray_background simple-table-header" style="width:132px"><em>Llama LLMs</em></th><td id="Id|b" class="block-color-gray_background" style="width:115px"></td><td id="Lt\b" class="block-color-gray_background" style="width:115px"></td><td id="zuMO" class="block-color-gray_background" style="width:115px"></td><td id="kqU`" class="block-color-gray_background" style="width:115px"></td><td id="K~q@" class="block-color-gray_background" style="width:115px"></td></tr><tr id="154a0c76-9fcd-8034-805b-fb1234881cdb"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px"><strong>    1B w/o AMT <br/>    embed init<br/></strong></th><td id="Id|b" class="" style="width:115px">1.0</td><td id="Lt\b" class="" style="width:115px">1.211</td><td id="zuMO" class="" style="width:115px">0.322</td><td id="kqU`" class="" style="width:115px">0.365 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.016</td><td id="K~q@" class="" style="width:115px">0.200 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-809d-8e98-e998e890a2e4"><th id="hmyS" class="block-color-brown_background simple-table-header" style="width:132px"><strong>    1B w/o AMT <br/>    embed init<br/></strong></th><td id="Id|b" class="block-color-brown_background" style="width:115px">1.1</td><td id="Lt\b" class="block-color-brown_background" style="width:115px">1.211</td><td id="zuMO" class="block-color-brown_background" style="width:115px">0.555</td><td id="kqU`" class="block-color-brown_background" style="width:115px">0.547 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.077</td><td id="K~q@" class="block-color-brown_background" style="width:115px">0.209 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-8035-88d6-d2894ae078a5"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px"><strong>    3B w/o AMT <br/>    embed init<br/></strong></th><td id="Id|b" class="" style="width:115px">1.0</td><td id="Lt\b" class="" style="width:115px">1.176</td><td id="zuMO" class="" style="width:115px">0.383</td><td id="kqU`" class="" style="width:115px">0.423 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.030</td><td id="K~q@" class="" style="width:115px">0.202 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-8075-846e-f3e4cb2762c4"><th id="hmyS" class="block-color-brown_background simple-table-header" style="width:132px"><strong>    3B w/o AMT <br/>    embed init<br/></strong></th><td id="Id|b" class="block-color-brown_background" style="width:115px">1.1</td><td id="Lt\b" class="block-color-brown_background" style="width:115px">1.176</td><td id="zuMO" class="block-color-brown_background" style="width:115px">0.271</td><td id="kqU`" class="block-color-brown_background" style="width:115px">0.331 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.008</td><td id="K~q@" class="block-color-brown_background" style="width:115px">0.210 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-807f-8afa-f6e5d23b59e4"><th id="hmyS" class="simple-table-header-color simple-table-header" style="width:132px"><strong>    3B w/ AMT <br/>    embed init<br/></strong></th><td id="Id|b" class="" style="width:115px">1.0</td><td id="Lt\b" class="" style="width:115px">1.181</td><td id="zuMO" class="" style="width:115px">0.342</td><td id="kqU`" class="" style="width:115px">0.401 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.017</td><td id="K~q@" class="" style="width:115px">0.203 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr><tr id="154a0c76-9fcd-8006-846c-d07ddc4392db"><th id="hmyS" class="block-color-brown_background simple-table-header" style="width:132px"><strong>    3B w/ AMT <br/>    embed init<br/></strong></th><td id="Id|b" class="block-color-brown_background" style="width:115px">1.1</td><td id="Lt\b" class="block-color-brown_background" style="width:115px">1.181</td><td id="zuMO" class="block-color-brown_background" style="width:115px"><strong>0.245</strong></td><td id="kqU`" class="block-color-brown_background" style="width:115px"><strong>0.280</strong> <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.018</td><td id="K~q@" class="block-color-brown_background" style="width:115px"><strong>0.215</strong> <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">±</span></span></span></span></span><span>﻿</span></span> 0.003</td></tr></tbody></table><p id="154a0c76-9fcd-80ce-bea8-de6b71095fb2" class="">All the quantitative evaluation results are included in the table above. To better position the performance of our models, we come up with a few <strong>baselines:</strong></p><ul id="154a0c76-9fcd-800a-921b-dc4a710e07fa" class="bulleted-list"><li style="list-style-type:disc"><strong>Ground truth:</strong> We measure the CLAP score between (text, music) pairs in the test set. This is meant to represent the <strong>best achievable</strong> <strong>text control</strong> of our trained models.</li></ul><ul id="154a0c76-9fcd-80d5-8a5d-d0a2748dc7a3" class="bulleted-list"><li style="list-style-type:disc"><strong>Ground truth (random text)</strong>: Similar to above, but we randomize the text descriptions in the test set. This gives us a sense of what CLAP score is achievable with <strong>no text control</strong>, when the music is <strong>human-composed</strong>.</li></ul><ul id="154a0c76-9fcd-80d5-8b8d-da2d5e0b7620" class="bulleted-list"><li style="list-style-type:disc"><strong>AMT-Large (unconditional)</strong>: We generate music with the unconditional AMT-Large model, and then pair the generated samples with texts in our test set. This tells us what <strong>machine-generated</strong> music with <strong>no text control </strong>would do on CLAP score — intuitively, if our models are achieve higher CLAP scores than this, it suggests that text conditioning is working to some extent.</li></ul><p id="154a0c76-9fcd-80e5-a665-d63ed610f831" class="">For apples-to-apples comparisons between all models, we now set aside the Llama LLM rows with generations using temperature 1.1 (i.e., those with a <mark class="highlight-brown_background">brown background</mark>). We first focus on comparing the two major model families, i.e., AMT vs. Llama. If we examine the FAD scores, we can see that <strong>AMT-Large + text conditioning</strong> performs better, scoring an FAD-sub of around 0.33 vs. 0.36~0.42 by Llama LLMs, which is a decently meaningful gap considering the standard errors. On the other hand, on CLAP score, <strong>Llama LLMs</strong> outperform the AMT models by a clear margin (i.e., 0.20 vs. 0.15), and all the models are in between the unconditional baseline (0.12) and paired ground truth (0.24), demonstrating different levels of text conditioning performance.</p><p id="154a0c76-9fcd-8027-9b28-cc416b4eab04" class="">Therefore, our general <strong>takeaways</strong> on the comparison between starting with a pretrained <strong>music</strong> or <strong>text</strong> backbone is that:</p><ul id="154a0c76-9fcd-80b9-b60a-f746802531e1" class="bulleted-list"><li style="list-style-type:disc">Starting with a <strong>music</strong> model (i.e., AMT-Large) would give us <strong>great generated music</strong>, which is reasonable as music is its pretraining domain, but the <strong>text control is relatively weak</strong>.</li></ul><ul id="154a0c76-9fcd-80af-8804-c971f27ef564" class="bulleted-list"><li style="list-style-type:disc">Starting with a <strong>text LLM</strong> (i.e., Llama 3.2) leads to <strong>more effective text control</strong>, but the <strong>generated music quality is not as good</strong>.</li></ul><p id="154a0c76-9fcd-8039-85cc-d7ac59e40730" class="">Given that controllability is a crucial aspect for generative AI tools to be useful, we would recommend starting with text LLMs and further explore techniques to enhance the music quality.</p><p id="154a0c76-9fcd-8097-83cb-ffa219978f50" class="">Now we dive into more fine-grained comparisons over our proposed<strong> techniques on providing better inductive biases</strong> for either AMT or Llama. We can see that on AMT-Large models, the use of weighted Llama text representations does not improve either the FAD or CLAP score. On the other hand, initializing the music token embeddings from pretrained AMT, which we experiment with the Llama 3B model, does improve FAD scores slightly, suggesting that this technique helps with the generated music quality.</p><p id="154a0c76-9fcd-8029-aaa7-faf978d2c4e0" class="">Lastly, we examine the effects with an <strong>increased temperature</strong> from 1.0 to 1.1 during generation, which we try on Llama 1B and 3B models. While both metrics improve in general with the higher temperature (except for FAD on the 1B model), upon listening to the samples, we find the music to be a lot more chaotic with little repetition structures typically found in human compositions. We suspect that the improved scores are due to better instrumentation coverage, i.e., the models are more willing to generate notes played by more instruments specified in the text prompt, and that the ML model-based FAD and CLAP metrics are more sensitive to instrumentation than to whether the musical content is sensible and well-structured. However, the exact mechanisms are to be investigated.</p><h2 id="151a0c76-9fcd-80e2-8888-e95f2834367b" class="">6.2. Qualitative study</h2><p id="157a0c76-9fcd-8048-9c1f-def6ebbfb28c" class="block-color-pink"><mark class="highlight-default">We listen to generated samples from the following two models:</mark></p><ul id="158a0c76-9fcd-8042-a752-c955c1f40c0f" class="bulleted-list"><li style="list-style-type:disc"><strong>AMT-Large</strong> <strong>w/ weighted Llama states</strong></li></ul><ul id="158a0c76-9fcd-801b-a4ca-c7cd0391c760" class="bulleted-list"><li style="list-style-type:disc"><strong>Llama 3.2 (3B)</strong> <strong>w/ AMT embedding initialization</strong></li></ul><p id="158a0c76-9fcd-80b6-b4ed-fef15330b5dc" class="">and include some of the generated music and the paired input text descriptions. For clarity, we mark parts in the text description that are being <strong>followed</strong> in the music in <mark class="highlight-teal"><strong>green</strong></mark><mark class="highlight-default">, and those being </mark><mark class="highlight-default"><strong>violated/ignored </strong></mark><mark class="highlight-default">in </mark><mark class="highlight-red"><strong>red</strong></mark><mark class="highlight-default">. The findings largely match those from our quantitative evaluation.</mark></p><p id="158a0c76-9fcd-800e-b22a-f37d367e93d8" class="">The AMT model, pretrained on music, exhibits great capabilities in generating high-quality music, through rich arrangements and well-tensioned, well-developed musical ideas. However, as can be seen from the large portions of text marked in <mark class="highlight-red">red</mark>, ineffective text control is its significant drawback.</p><p id="158a0c76-9fcd-80a8-a4f1-f644e4ad604d" class="">In contrast, the 3B Llama model, pretrained on (a much larger volume of) text, boasts great text control (text descriptions mostly marked in <mark class="highlight-teal">green</mark>), getting the vibes and instrumentation correct most of the time. Yet, the music it generates are mostly simple, with significantly fewer twists and turns compared to the AMT model, and thus much less catchy. Also, when we attempt to increase the temperature (1.0 → 1.1), the outputs immediately sound more chaotic and unstructured, rather than being richer and more diverse as we desire.</p><p id="158a0c76-9fcd-8035-bbdd-d7021a419b1d" class="">
</p><p id="157a0c76-9fcd-80b9-8910-f4e51a27db5a" class=""><strong>AMT-Large with weighted Llama states</strong></p>
<!-- <figure id="158a0c76-9fcd-80dc-a78b-dd97b0e50ab4"><div class="source"> -->
	<!-- <a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/f4ab350c9d732fdbccbb9109335035c8.mp3">
		https://prod-files-secure.s3.us-west-2.amazonaws.com/c7a62813-4265-4e43-a058-7c5f8532f9e0/668e0aa4-3e5c-42ae-905d-a3aa0776097c/f4ab350c9d732fdbccbb9109335035c8.mp3</a> -->

	
<!-- <style>

	audio {
		margin: 0;
		padding: 0;
	}
</style> -->


<div class="center-audio">
<audio controls style="display: block; margin: 0 auto; outline: none;">
	<source src="LLM-based Finetuning for Text to Symbolic Music Ge 151a0c769fcd80389f67cabf1a296416/f4ab350c9d732fdbccbb9109335035c8.mp3" type="audio/mp3">
	Your browser does not support the audio element.
</audio>
</div>

				
<!-- <a href="LLM-based Finetuning for Text to Symbolic Music Ge 151a0c769fcd80389f67cabf1a296416/f4ab350c9d732fdbccbb9109335035c8.mp3"></a> -->

<figure id="158a0c76-9fcd-808f-8e56-fb9cac4bc3bd" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.16.28_PM.png"><img style="width:708px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.16.28_PM.png"/></a><figcaption>AMT Sample 01</figcaption></figure>

<!-- <figure id="158a0c76-9fcd-8081-8c60-cb8d55fb240a"><div class="source"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/f6c5a4e607da3f80d24cb70a28ddcb26.mp3">https://prod-files-secure.s3.us-west-2.amazonaws.com/c7a62813-4265-4e43-a058-7c5f8532f9e0/3878491e-0f45-4a51-bd82-b9b04e8d557e/f6c5a4e607da3f80d24cb70a28ddcb26.mp3</a></div></figure> -->
<div class="center-audio">
	<audio controls style="display: block; margin: 0 auto; outline: none;">
		<source src="LLM-based Finetuning for Text to Symbolic Music Ge 151a0c769fcd80389f67cabf1a296416/f6c5a4e607da3f80d24cb70a28ddcb26.mp3" type="audio/mp3">
		Your browser does not support the audio element.
	</audio>
</div>

<figure id="158a0c76-9fcd-8028-aa44-e78a63a8a499" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.12.18_PM.png"><img style="width:708px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.12.18_PM.png"/></a><figcaption>AMT Sample 02</figcaption></figure><p id="4055c77e-2307-46f4-8c7d-31be5fcfda79" class=""><strong>Llama 3.2 (3B) with AMT embedding initialization, temperature = 1.0</strong></p>

<!-- <figure id="158a0c76-9fcd-800c-8036-c52528e3a1fa"><div class="source"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/f0bb5527ec2aae056f4e0ec18ea36616.mp3">https://prod-files-secure.s3.us-west-2.amazonaws.com/c7a62813-4265-4e43-a058-7c5f8532f9e0/586bf6f9-436a-47a8-b5d4-1fd14960a268/f0bb5527ec2aae056f4e0ec18ea36616.mp3</a></div></figure>
 
-->
<div class="center-audio">
	<audio controls style="display: block; margin: 0 auto; outline: none;">
		<source src="LLM-based Finetuning for Text to Symbolic Music Ge 151a0c769fcd80389f67cabf1a296416/f0bb5527ec2aae056f4e0ec18ea36616.mp3" type="audio/mp3">
		Your browser does not support the audio element.
	</audio>
</div>
<figure id="158a0c76-9fcd-803a-8fb6-f64f218f718d" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.12.37_PM.png"><img style="width:708px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.12.37_PM.png"/></a><figcaption>Llama Sample 01</figcaption></figure>

<!-- <figure id="158a0c76-9fcd-8063-8f9f-ebf3b21e3b1d"><div class="source"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/f1acf9af19f4121a24b8f6329190e0b4.mp3">https://prod-files-secure.s3.us-west-2.amazonaws.com/c7a62813-4265-4e43-a058-7c5f8532f9e0/aefa9f46-97ff-4791-88ae-d74ccfdd89c1/f1acf9af19f4121a24b8f6329190e0b4.mp3</a></div></figure> -->
<div class="center-audio">
	<audio controls style="display: block; margin: 0 auto; outline: none;">
		<source src="LLM-based Finetuning for Text to Symbolic Music Ge 151a0c769fcd80389f67cabf1a296416/f1acf9af19f4121a24b8f6329190e0b4.mp3" type="audio/mp3">
		Your browser does not support the audio element.
	</audio>
</div>

<figure id="158a0c76-9fcd-806d-9359-e550dbf27c3f" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.12.58_PM.png"><img style="width:707.9609375px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.12.58_PM.png"/></a><figcaption>Llama Sample 02</figcaption></figure><p id="151a0c76-9fcd-804c-ae1e-e1dbae76e291" class=""><strong>Llama 3.2 (3B) with AMT embedding initialization, temperature = 1.1</strong></p>

<!-- <figure id="158a0c76-9fcd-8092-9ff5-e8df929de875"><div class="source"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/f0b195d6ff022fce020b20742424e190.mp3">https://prod-files-secure.s3.us-west-2.amazonaws.com/c7a62813-4265-4e43-a058-7c5f8532f9e0/f68c59e7-cd73-48ce-9c52-30a12d7f96a3/f0b195d6ff022fce020b20742424e190.mp3</a></div></figure> -->
<div class="center-audio">
	<audio controls style="display: block; margin: 0 auto; outline: none;">
		<source src="LLM-based Finetuning for Text to Symbolic Music Ge 151a0c769fcd80389f67cabf1a296416/f0b195d6ff022fce020b20742424e190.mp3" type="audio/mp3">
		Your browser does not support the audio element.
	</audio>
</div>

<figure id="158a0c76-9fcd-80fe-94f0-df019acf7cd4" class="image"><a href="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.13.19_PM.png"><img style="width:708px" src="LLM-based%20Finetuning%20for%20Text%20to%20Symbolic%20Music%20Ge%20151a0c769fcd80389f67cabf1a296416/Screenshot_2024-12-09_at_9.13.19_PM.png"/></a><figcaption>Llama (temp = 1.1) Sample 01</figcaption></figure><h1 id="151a0c76-9fcd-8014-8cf0-c4d3fd851fd2" class="">7. Limitations and Future Work</h1><h3 id="155a0c76-9fcd-80bb-8c9d-ccaec6b46fb7" class="">Overfitting issues</h3><p id="155a0c76-9fcd-804e-aea9-f9da52e3bbe3" class="">As we observed in the experiments (see <mark class="highlight-orange">Figure 3</mark>), all Llama 3.2 (1B and 3B) text LLMs exhibit serious levels of overfitting when fine-tuned on music data. This likely has resulted from a combination of too little fine-tuning data, and too many trainable parameters. As future work, on the data front, following the strategy of <em>Textbooks are all you need</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Gunasekar%20et%20al%20,%202023%20155a0c769fcd814082f7df60d6061cc3.html">Gunasekar et al., 2023</a>), we can consider adding music-domain continued pretraining data to equip the text LLM with more generalizable musical knowledge before fine-tuning on music generation. Meanwhile, on the modeling front, we can experiment with low-rank adaptation methods, e.g., LoRA (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Hu%20et%20al%20,%202021%20155a0c769fcd8190b5e6d30029e29fa0.html">Hu et al., 2021</a>) and QLoRA (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Dettmers%20et%20al%20,%202023%20155a0c769fcd813e96e7e5f71fcd3267.html">Dettmers et al., 2023</a>) to reduce the number of trainable parameters, and potentially scale to larger (frozen) backbones (e.g., 8B or 70B Llama models) as a stronger starting point.</p><h3 id="155a0c76-9fcd-806d-8f9c-d297c648b9da" class="">Short sequence length</h3><p id="155a0c76-9fcd-80fa-af0b-ee5a2c9ab6b0" class="">Due to resource constraints, we chose 1024 as our training sequence length, which is roughly only equal to 15 seconds of music. The issues with this approach are multifold:</p><ul id="155a0c76-9fcd-807a-bb4d-c317519ac8ca" class="bulleted-list"><li style="list-style-type:disc">Some longer-term repetition structures, e.g., motivic development or verse-chorus forms, often take more than 15 seconds to unfold. With enough time and resources, we should increase the training sequence length to 4K (i.e., 1 minute) or 16K (i.e., roughly entire songs), for the model to better capture the aspects above.</li></ul><ul id="155a0c76-9fcd-8031-8f71-f846b1ab62da" class="bulleted-list"><li style="list-style-type:disc">The text descriptions in <em>MidiCaps</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Melechovsky%20et%20al%20,%202024%20151a0c769fcd8114ad24c0bc93f85ade.html">Melechovsky et al., 2024</a>) are associated with full songs, hence are likely less accurate on crops of songs. This can be resolved if we train on full songs. However, ideally, we would prefer a model that can generate short excerpts, e.g., 10 seconds, based on text descriptions describing highly localized attributes, e.g., “<code>melody pitches going up with fragmented dense rhythm patterns</code> ”. To obtain such training data pairs, we can leverage audio multimodal LLMs, e.g., <em>Qwen2-audio</em> (<a href="References%2014ea0c769fcd80198d65d16d264c3e46/Chu%20et%20al%20,%202024%20155a0c769fcd8177a1b9ed66b8681fb2.html">Chu et al., 2024</a>), to produce text descriptions for music of different durations, and apply some filtering mechanisms to ensure data quality.</li></ul><h1 id="151a0c76-9fcd-80fe-a89e-f31603065600" class="">References</h1><div id="158a0c76-9fcd-8035-b2f7-ea3bdbeef7d1" class="collection-content"><h4 class="collection-title">References</h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesTitle"><path d="M0.637695 13.1914C1.0957 13.1914 1.32812 13 1.47852 12.5215L2.24414 10.3887H6.14746L6.90625 12.5215C7.05664 13 7.2959 13.1914 7.74707 13.1914C8.22559 13.1914 8.5332 12.9043 8.5332 12.4531C8.5332 12.2891 8.50586 12.1523 8.44434 11.9678L5.41602 3.79199C5.2041 3.21777 4.82129 2.9375 4.19922 2.9375C3.60449 2.9375 3.21484 3.21777 3.0166 3.78516L-0.0322266 12.002C-0.09375 12.1797 -0.121094 12.3232 -0.121094 12.4668C-0.121094 12.918 0.166016 13.1914 0.637695 13.1914ZM2.63379 9.12402L4.17871 4.68066H4.21973L5.76465 9.12402H2.63379ZM12.2793 13.2324C13.3115 13.2324 14.2891 12.6787 14.7129 11.8037H14.7402V12.5762C14.7471 12.9863 15.0273 13.2393 15.4238 13.2393C15.834 13.2393 16.1143 12.9795 16.1143 12.5215V8.00977C16.1143 6.49902 14.9658 5.52148 13.1543 5.52148C11.7666 5.52148 10.6592 6.08887 10.2695 6.99121C10.1943 7.15527 10.1533 7.3125 10.1533 7.46289C10.1533 7.81152 10.4062 8.04395 10.7686 8.04395C11.0215 8.04395 11.2129 7.94824 11.3496 7.73633C11.7529 6.99121 12.2861 6.65625 13.1064 6.65625C14.0977 6.65625 14.6992 7.20996 14.6992 8.1123V8.67285L12.5664 8.7959C10.7686 8.8916 9.77734 9.69824 9.77734 11.0107C9.77734 12.3369 10.8096 13.2324 12.2793 13.2324ZM12.6621 12.1387C11.8008 12.1387 11.2129 11.667 11.2129 10.9561C11.2129 10.2725 11.7598 9.82129 12.7578 9.75977L14.6992 9.62988V10.3203C14.6992 11.3457 13.7969 12.1387 12.6621 12.1387Z"></path></svg></span>Name</th><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesText"><path d="M1.56738 3.25879H14.4258C14.7676 3.25879 15.0479 2.97852 15.0479 2.63672C15.0479 2.29492 14.7744 2.02148 14.4258 2.02148H1.56738C1.21875 2.02148 0.952148 2.29492 0.952148 2.63672C0.952148 2.97852 1.22559 3.25879 1.56738 3.25879ZM1.56738 6.84082H14.4258C14.7676 6.84082 15.0479 6.56055 15.0479 6.21875C15.0479 5.87695 14.7744 5.60352 14.4258 5.60352H1.56738C1.21875 5.60352 0.952148 5.87695 0.952148 6.21875C0.952148 6.56055 1.22559 6.84082 1.56738 6.84082ZM1.56738 10.4229H14.4258C14.7676 10.4229 15.0479 10.1426 15.0479 9.80078C15.0479 9.45898 14.7744 9.18555 14.4258 9.18555H1.56738C1.21875 9.18555 0.952148 9.45898 0.952148 9.80078C0.952148 10.1426 1.22559 10.4229 1.56738 10.4229ZM1.56738 14.0049H8.75879C9.10059 14.0049 9.38086 13.7246 9.38086 13.3828C9.38086 13.041 9.10742 12.7676 8.75879 12.7676H1.56738C1.21875 12.7676 0.952148 13.041 0.952148 13.3828C0.952148 13.7246 1.22559 14.0049 1.56738 14.0049Z"></path></svg></span>Title</th><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesText"><path d="M1.56738 3.25879H14.4258C14.7676 3.25879 15.0479 2.97852 15.0479 2.63672C15.0479 2.29492 14.7744 2.02148 14.4258 2.02148H1.56738C1.21875 2.02148 0.952148 2.29492 0.952148 2.63672C0.952148 2.97852 1.22559 3.25879 1.56738 3.25879ZM1.56738 6.84082H14.4258C14.7676 6.84082 15.0479 6.56055 15.0479 6.21875C15.0479 5.87695 14.7744 5.60352 14.4258 5.60352H1.56738C1.21875 5.60352 0.952148 5.87695 0.952148 6.21875C0.952148 6.56055 1.22559 6.84082 1.56738 6.84082ZM1.56738 10.4229H14.4258C14.7676 10.4229 15.0479 10.1426 15.0479 9.80078C15.0479 9.45898 14.7744 9.18555 14.4258 9.18555H1.56738C1.21875 9.18555 0.952148 9.45898 0.952148 9.80078C0.952148 10.1426 1.22559 10.4229 1.56738 10.4229ZM1.56738 14.0049H8.75879C9.10059 14.0049 9.38086 13.7246 9.38086 13.3828C9.38086 13.041 9.10742 12.7676 8.75879 12.7676H1.56738C1.21875 12.7676 0.952148 13.041 0.952148 13.3828C0.952148 13.7246 1.22559 14.0049 1.56738 14.0049Z"></path></svg></span>Authors</th></tr></thead><tbody><tr id="151a0c76-9fcd-818f-87d3-e3dc7f5fcb2f"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Grattafiori%20et%20al%20,%202024%20151a0c769fcd818f87d3e3dc7f5fcb2f.html">Grattafiori et al., 2024</a></td><td class="cell-QE=d">The Llama 3 Herd of Models</td><td class="cell-RaJW">Grattafiori, Aaron<br/>Dubey, Abhimanyu<br/>Jauhri, Abhinav<br/>Pandey, Abhinav<br/>Kadian, Abhishek et al.<br/></td></tr><tr id="151a0c76-9fcd-8114-ad24-c0bc93f85ade"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Melechovsky%20et%20al%20,%202024%20151a0c769fcd8114ad24c0bc93f85ade.html">Melechovsky et al., 2024</a></td><td class="cell-QE=d">MidiCaps: A large-scale MIDI dataset with text captions</td><td class="cell-RaJW">Melechovsky, Jan<br/>Roy, Abhinaba<br/>Herremans, Dorien<br/></td></tr><tr id="151a0c76-9fcd-81f0-ac8b-c7340b768259"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Thickstun%20et%20al%20,%202024%20151a0c769fcd81f0ac8bc7340b768259.html">Thickstun et al., 2024</a></td><td class="cell-QE=d">Anticipatory Music Transformer</td><td class="cell-RaJW">Thickstun, John<br/>Hall, David<br/>Donahue, Chris<br/>Liang, Percy<br/></td></tr><tr id="151a0c76-9fcd-813b-8e9f-f830924821e4"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Agostinelli%20et%20al%20,%202023%20151a0c769fcd813b8e9ff830924821e4.html">Agostinelli et al., 2023</a></td><td class="cell-QE=d">MusicLM: Generating Music From Text</td><td class="cell-RaJW">Agostinelli, Andrea<br/>Denk, Timo I.<br/>Borsos, Zalán<br/>Engel, Jesse<br/>Verzetti, Mauro<br/>Caillon, Antoine<br/>Huang, Qingqing<br/>Jansen, Aren<br/>Roberts, Adam<br/>Tagliasacchi, Marco<br/>Sharifi, Matt<br/>Zeghidour, Neil<br/>Frank, Christian<br/></td></tr><tr id="151a0c76-9fcd-8182-8738-f72691b1fb6f"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Copet%20et%20al%20,%202023%20151a0c769fcd81828738f72691b1fb6f.html">Copet et al., 2023</a></td><td class="cell-QE=d">Simple and Controllable Music Generation</td><td class="cell-RaJW">Copet, Jade<br/>Kreuk, Felix<br/>Gat, Itai<br/>Remez, Tal<br/>Kant, David<br/>Synnaeve, Gabriel<br/>Adi, Yossi<br/>Défossez, Alexandre<br/></td></tr><tr id="151a0c76-9fcd-8168-bdd6-d97d8467939a"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Evans%20et%20al%20,%202024%20151a0c769fcd8168bdd6d97d8467939a.html">Evans et al., 2024</a></td><td class="cell-QE=d">Stable Audio Open</td><td class="cell-RaJW">Evans, Zach<br/>Parker, Julian D.<br/>Carr, C. J.<br/>Zukowski, Zack<br/>Taylor, Josiah<br/>Pons, Jordi<br/></td></tr><tr id="151a0c76-9fcd-81d2-8c39-c5c6cb5540a6"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Deng%20et%20al%20,%202024%20151a0c769fcd81d28c39c5c6cb5540a6.html">Deng et al., 2024</a></td><td class="cell-QE=d">ComposerX: Multi-Agent Symbolic Music Composition with LLMs</td><td class="cell-RaJW">Deng, Qixin<br/>Yang, Qikai<br/>Yuan, Ruibin<br/>Huang, Yipeng<br/>Wang, Yi<br/>Liu, Xubo<br/>Tian, Zeyue<br/>Pan, Jiahao<br/>Zhang, Ge<br/>Lin, Hanfeng<br/>Li, Yizhi<br/>Ma, Yinghao<br/>Fu, Jie<br/>Lin, Chenghua<br/>Benetos, Emmanouil<br/>Wang, Wenwu<br/>Xia, Guangyu<br/>Xue, Wei<br/>Guo, Yike<br/></td></tr><tr id="151a0c76-9fcd-8124-8e6f-dc80d28471dc"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Yuan%20et%20al%20,%202024%20151a0c769fcd81248e6fdc80d28471dc.html">Yuan et al., 2024</a></td><td class="cell-QE=d">ChatMusician: Understanding and Generating Music Intrinsically with LLM</td><td class="cell-RaJW">Yuan, Ruibin<br/>Lin, Hanfeng<br/>Wang, Yi<br/>Tian, Zeyue<br/>Wu, Shangda<br/>Shen, Tianhao<br/>Zhang, Ge<br/>Wu, Yuhang<br/>Liu, Cong<br/>Zhou, Ziya<br/>Ma, Ziyang<br/>Xue, Liumeng<br/>Wang, Ziyu<br/>Liu, Qin<br/>Zheng, Tianyu<br/>Li, Yizhi<br/>Ma, Yinghao<br/>Liang, Yiming<br/>Chi, Xiaowei<br/>Liu, Ruibo<br/>Wang, Zili<br/>Li, Pengfei<br/>Wu, Jingcheng<br/>Lin, Chenghua<br/>Liu, Qifeng<br/>Jiang, Tao<br/>Huang, Wenhao<br/>Chen, Wenhu<br/>Benetos, Emmanouil<br/>Fu, Jie<br/>Xia, Gus<br/>Dannenberg, Roger<br/>Xue, Wei<br/>Kang, Shiyin<br/>Guo, Yike<br/></td></tr><tr id="151a0c76-9fcd-810a-b31f-f2cfca3a1624"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Kilgour%20et%20al%20,%202019%20151a0c769fcd810ab31ff2cfca3a1624.html">Kilgour et al., 2019</a></td><td class="cell-QE=d">Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms</td><td class="cell-RaJW">Kilgour, Kevin<br/>Zuluaga, Mauricio<br/>Roblek, Dominik<br/>Sharifi, Matthew<br/></td></tr><tr id="151a0c76-9fcd-81d0-9d65-e353e5a3453d"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Wu%20et%20al%20,%202023%20151a0c769fcd81d09d65e353e5a3453d.html">Wu et al., 2023</a></td><td class="cell-QE=d">Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</td><td class="cell-RaJW">Wu, Yusong<br/>Chen, Ke<br/>Zhang, Tianyu<br/>Hui, Yuchen<br/>Nezhurina, Marianna<br/>Berg-Kirkpatrick, Taylor<br/>Dubnov, Shlomo<br/></td></tr><tr id="151a0c76-9fcd-8133-b27c-f06af4a0b23f"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/%E2%81%A8Wu%E2%81%A9%20and%20%E2%81%A8Yang%E2%81%A9,%202023%20151a0c769fcd8133b27cf06af4a0b23f.html">⁨Wu⁩ and ⁨Yang⁩, 2023</a></td><td class="cell-QE=d">Compose &amp; Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach</td><td class="cell-RaJW">Wu, Shih-Lun<br/>Yang, Yi-Hsuan<br/></td></tr><tr id="152a0c76-9fcd-815e-804f-eab17736b06c"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/%E2%81%A8Loshchilov%E2%81%A9%20and%20%E2%81%A8Hutter%E2%81%A9,%202019%20152a0c769fcd815e804feab17736b06c.html">⁨Loshchilov⁩ and ⁨Hutter⁩, 2019</a></td><td class="cell-QE=d">Decoupled Weight Decay Regularization</td><td class="cell-RaJW">Loshchilov, Ilya<br/>Hutter, Frank<br/></td></tr><tr id="152a0c76-9fcd-8130-8c68-fe5c1a5c8724"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Dao,%202023%20152a0c769fcd81308c68fe5c1a5c8724.html">Dao, 2023</a></td><td class="cell-QE=d">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</td><td class="cell-RaJW">Dao, Tri</td></tr><tr id="152a0c76-9fcd-810b-abcb-f934793b7b22"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Holtzman%20et%20al%20,%202020%20152a0c769fcd810babcbf934793b7b22.html">Holtzman et al., 2020</a></td><td class="cell-QE=d">The Curious Case of Neural Text Degeneration</td><td class="cell-RaJW">Holtzman, Ari<br/>Buys, Jan<br/>Du, Li<br/>Forbes, Maxwell<br/>Choi, Yejin<br/></td></tr><tr id="155a0c76-9fcd-8184-8954-e03ff70f3183"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Huang%20et%20al%20,%202018%20155a0c769fcd81848954e03ff70f3183.html">Huang et al., 2018</a></td><td class="cell-QE=d">Music Transformer</td><td class="cell-RaJW">Huang, Cheng-Zhi Anna<br/>Vaswani, Ashish<br/>Uszkoreit, Jakob<br/>Shazeer, Noam<br/>Simon, Ian<br/>Hawthorne, Curtis<br/>Dai, Andrew M.<br/>Hoffman, Matthew D.<br/>Dinculescu, Monica<br/>Eck, Douglas<br/></td></tr><tr id="155a0c76-9fcd-813e-96e7-e5f71fcd3267"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Dettmers%20et%20al%20,%202023%20155a0c769fcd813e96e7e5f71fcd3267.html">Dettmers et al., 2023</a></td><td class="cell-QE=d">QLoRA: Efficient Finetuning of Quantized LLMs</td><td class="cell-RaJW">Dettmers, Tim<br/>Pagnoni, Artidoro<br/>Holtzman, Ari<br/>Zettlemoyer, Luke<br/></td></tr><tr id="155a0c76-9fcd-8190-b5e6-d30029e29fa0"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Hu%20et%20al%20,%202021%20155a0c769fcd8190b5e6d30029e29fa0.html">Hu et al., 2021</a></td><td class="cell-QE=d">LoRA: Low-Rank Adaptation of Large Language Models</td><td class="cell-RaJW">Hu, Edward J.<br/>Shen, Yelong<br/>Wallis, Phillip<br/>Allen-Zhu, Zeyuan<br/>Li, Yuanzhi<br/>Wang, Shean<br/>Wang, Lu<br/>Chen, Weizhu<br/></td></tr><tr id="155a0c76-9fcd-8140-82f7-df60d6061cc3"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Gunasekar%20et%20al%20,%202023%20155a0c769fcd814082f7df60d6061cc3.html">Gunasekar et al., 2023</a></td><td class="cell-QE=d">Textbooks Are All You Need</td><td class="cell-RaJW">Gunasekar, Suriya<br/>Zhang, Yi<br/>Aneja, Jyoti<br/>Mendes, Caio César Teodoro<br/>Giorno, Allie Del<br/>Gopi, Sivakanth<br/>Javaheripi, Mojan<br/>Kauffmann, Piero<br/>Rosa, Gustavo de<br/>Saarikivi, Olli<br/>Salim, Adil<br/>Shah, Shital<br/>Behl, Harkirat Singh<br/>Wang, Xin<br/>Bubeck, Sébastien<br/>Eldan, Ronen<br/>Kalai, Adam Tauman<br/>Lee, Yin Tat<br/>Li, Yuanzhi<br/></td></tr><tr id="155a0c76-9fcd-8177-a1b9-ed66b8681fb2"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Chu%20et%20al%20,%202024%20155a0c769fcd8177a1b9ed66b8681fb2.html">Chu et al., 2024</a></td><td class="cell-QE=d">Qwen2-Audio Technical Report</td><td class="cell-RaJW">Chu, Yunfei<br/>Xu, Jin<br/>Yang, Qian<br/>Wei, Haojie<br/>Wei, Xipin<br/>Guo, Zhifang<br/>Leng, Yichong<br/>Lv, Yuanjun<br/>He, Jinzheng<br/>Lin, Junyang<br/>Zhou, Chang<br/>Zhou, Jingren<br/></td></tr><tr id="157a0c76-9fcd-81dc-9dc4-cd7e1021dffa"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/De%CC%81fossez%20et%20al%20,%202022%20157a0c769fcd81dc9dc4cd7e1021dffa.html">Défossez et al., 2022</a></td><td class="cell-QE=d">High Fidelity Neural Audio Compression</td><td class="cell-RaJW">Défossez, Alexandre<br/>Copet, Jade<br/>Synnaeve, Gabriel<br/>Adi, Yossi<br/></td></tr><tr id="157a0c76-9fcd-819c-9e06-d43fa9e68a79"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Choi%20et%20al%20,%202017%20157a0c769fcd819c9e06d43fa9e68a79.html">Choi et al., 2017</a></td><td class="cell-QE=d">A Tutorial on Deep Learning for Music Information Retrieval</td><td class="cell-RaJW">Choi, Keunwoo<br/>Fazekas, György<br/>Cho, Kyunghyun<br/>Sandler, Mark<br/></td></tr><tr id="157a0c76-9fcd-8116-b338-e927a0481d94"><td class="cell-title"><a href="References%2014ea0c769fcd80198d65d16d264c3e46/Wei%20et%20al%20,%202022%20157a0c769fcd8116b338e927a0481d94.html">Wei et al., 2022</a></td><td class="cell-QE=d">Finetuned Language Models Are Zero-Shot Learners</td><td class="cell-RaJW">Wei, Jason<br/>Bosma, Maarten<br/>Zhao, Vincent Y.<br/>Guu, Kelvin<br/>Yu, Adams Wei<br/>Lester, Brian<br/>Du, Nan<br/>Dai, Andrew M.<br/>Le, Quoc V.<br/></td></tr></tbody></table><br/><br/></div><p id="151a0c76-9fcd-808d-9cb7-ccee0b30774d" class="">
</p><p id="151a0c76-9fcd-8022-a42b-e9fa2f5afa76" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>